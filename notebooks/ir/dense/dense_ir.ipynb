{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "787e1675",
   "metadata": {},
   "source": [
    "## Dense IR\n",
    "In this notebook, we show how to train a model, index data, and run search using Neural IR.\n",
    "\n",
    "In orded to run (almost) instantaneously, we use trivial data sizes of training data and collection to search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d64fe65",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "First, we need to include the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fa777e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    " \n",
    "from oneqa.ir.dense.colbert_top.colbert.utils.utils import create_directory, print_message\n",
    "from oneqa.ir.dense.colbert_top.colbert.infra import Run, RunConfig\n",
    "from oneqa.ir.dense.colbert_top.colbert.infra.config import ColBERTConfig\n",
    "from oneqa.ir.dense.colbert_top.colbert.training.training import train\n",
    "from oneqa.ir.dense.colbert_top.colbert.indexing.collection_indexer import encode\n",
    "from oneqa.ir.dense.colbert_top.colbert.searcher import Searcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322f5139",
   "metadata": {},
   "source": [
    "## Training\n",
    "We will train a ColBERT model using a TSVfile containing [query, positive document, negative document] triples.\n",
    "\n",
    "The path in `test_files_location` below points to the location of files used by the notebook, by default it poits to the files used by CI testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23f199e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files_location = '../../../tests/resources/ir_dense'\n",
    "model_type = 'xlm-roberta-base'\n",
    "with tempfile.TemporaryDirectory() as working_dir:\n",
    "    output_dir=os.path.join(working_dir, 'output_dir')\n",
    "text_triples_fn = os.path.join(test_files_location, \"xorqa.train_ir_negs_5_poss_1_001pct_at_0pct.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3eec7578",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>중국에서 가장 오랜기간 왕위를 유지한 인물은 누구인가?</td>\n",
       "      <td>Kangxi Emperor The Kangxi Emperor's reign of 61 years makes him the longest-reigning emperor in Chinese history (although his grandson, the Qianlong Emperor, had the longest period of \"de facto\" power) and one of the longest-reigning rulers in the world. However, since he ascended the throne at the age of seven, actual power was held for six years by four regents and his grandmother, the Grand Empress Dowager Xiaozhuang.</td>\n",
       "      <td>Chiddy Bang new songs from the duo and in November 2009 debuted the group's first free mixtape entitled \"The Swelly Express\". On 28 April 2011 during the first-ever MTV O Music Awards, Anamege broke the Guinness World Record for Longest Freestyle Rap and Longest Marathon Rapping Record by freestyling for 9 hours, 18 minutes, and 22 seconds, stealing the throne from rapper M-Eighty, who originally broke the record in 2009 by rapping for 9 hours, 15 minutes and 15 seconds. Anamege had also beat Canadian rapper D.O. for Longest Marathon Rapping session, the previous record being for 8 hours and 45 minutes.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "data = pd.read_csv(text_triples_fn, sep='\\t', nrows=1, header=None)\n",
    "display(HTML(data.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf6c9b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict = {\n",
    "                'root': output_dir,\n",
    "                'experiment': 'test_training',\n",
    "                'triples': text_triples_fn,\n",
    "                'similarity': 'l2',\n",
    "                'model_type': model_type,\n",
    "                'maxsteps': 3,\n",
    "                'bsize': 1,\n",
    "                'accumsteps': 1,\n",
    "                'amp': True,\n",
    "                'epochs': 1,\n",
    "                'rank': 0,\n",
    "                'nranks': 1\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f9433a",
   "metadata": {},
   "source": [
    "Next we train the model, and save it's location in `the latest_model_fn`variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eba19fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"nprobe\": 2,\n",
      "    \"ncandidates\": 8192,\n",
      "    \"index_path\": null,\n",
      "    \"nbits\": 1,\n",
      "    \"kmeans_niters\": 20,\n",
      "    \"num_partitions_max\": 10000000,\n",
      "    \"similarity\": \"l2\",\n",
      "    \"bsize\": 1,\n",
      "    \"accumsteps\": 1,\n",
      "    \"lr\": 3e-6,\n",
      "    \"maxsteps\": 3,\n",
      "    \"save_every\": null,\n",
      "    \"resume\": false,\n",
      "    \"resume_optimizer\": false,\n",
      "    \"warmup\": null,\n",
      "    \"warmup_bert\": null,\n",
      "    \"relu\": false,\n",
      "    \"nway\": 2,\n",
      "    \"use_ib_negatives\": false,\n",
      "    \"reranker\": false,\n",
      "    \"distillation_alpha\": 1.0,\n",
      "    \"ignore_scores\": false,\n",
      "    \"shuffle_every_epoch\": false,\n",
      "    \"save_steps\": 2000,\n",
      "    \"save_epochs\": -1,\n",
      "    \"epochs\": 1,\n",
      "    \"input_arguments\": {},\n",
      "    \"model_type\": \"xlm-roberta-base\",\n",
      "    \"init_from_lm\": null,\n",
      "    \"local_models_repository\": null,\n",
      "    \"ranks_fn\": null,\n",
      "    \"topK\": 100,\n",
      "    \"student_teacher_temperature\": 1.0,\n",
      "    \"student_teacher_top_loss_weight\": 0.5,\n",
      "    \"teacher_model_type\": \"xlm-roberta-base\",\n",
      "    \"teacher_doc_maxlen\": 180,\n",
      "    \"distill_query_passage_separately\": false,\n",
      "    \"query_only\": false,\n",
      "    \"loss_function\": null,\n",
      "    \"query_weight\": 0.5,\n",
      "    \"query_maxlen\": 32,\n",
      "    \"attend_to_mask_tokens\": false,\n",
      "    \"interaction\": \"colbert\",\n",
      "    \"dim\": 128,\n",
      "    \"doc_maxlen\": 220,\n",
      "    \"mask_punctuation\": true,\n",
      "    \"checkpoint\": null,\n",
      "    \"teacher_checkpoint\": null,\n",
      "    \"triples\": \"..\\/..\\/..\\/tests\\/resources\\/ir_dense\\/xorqa.train_ir_negs_5_poss_1_001pct_at_0pct.tsv\",\n",
      "    \"teacher_triples\": null,\n",
      "    \"collection\": null,\n",
      "    \"queries\": null,\n",
      "    \"index_name\": null,\n",
      "    \"overwrite\": false,\n",
      "    \"root\": \"\\/tmp\\/tmpol5v7ufl\\/output_dir\",\n",
      "    \"experiment\": \"test_training\",\n",
      "    \"index_root\": null,\n",
      "    \"name\": \"2022-05\\/05\\/11.45.43\",\n",
      "    \"rank\": 0,\n",
      "    \"nranks\": 1,\n",
      "    \"amp\": true,\n",
      "    \"gpus\": 0\n",
      "}\n",
      "[May 05, 11:45:47] model type: xlm-roberta-base\n",
      "[May 05, 11:45:47] Using config.bsize = 1 (per process) and config.accumsteps = 1\n",
      "[May 05, 11:45:47] get query model type: xlm-roberta-base\n",
      "[May 05, 11:45:48] get doc model type: xlm-roberta-base\n",
      "[May 05, 11:45:50] #> Loading triples...\n",
      "[May 05, 11:45:50] #>>>>> at ColBERT name (model type) : xlm-roberta-base\n",
      "[May 05, 11:45:50] #>>>>> at BaseColBERT name (model type) : xlm-roberta-base\n",
      "[May 05, 11:45:50] #> base_config.py load_from_checkpoint xlm-roberta-base\n",
      "[May 05, 11:45:50] #> base_config.py load_from_checkpoint xlm-roberta-base/artifact.metadata\n",
      "[May 05, 11:45:50] factory model type: xlm-roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing HF_ColBERT_XLMR: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing HF_ColBERT_XLMR from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing HF_ColBERT_XLMR from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of HF_ColBERT_XLMR were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'linear.weight', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.pooler.dense.weight', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.embeddings.token_type_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.pooler.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.embeddings.word_embeddings.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.key.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[May 05, 11:46:01] maxsteps: 3\n",
      "[May 05, 11:46:01] 1 epochs of 5 examples\n",
      "[May 05, 11:46:01] batch size: 1\n",
      "[May 05, 11:46:01] maxsteps set to 3\n",
      "[May 05, 11:46:01] start batch idx: 0\n",
      "[May 05, 11:46:01] #> XMLR QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
      "[May 05, 11:46:01] #> Input: $ 중국에서 가장 오랜기간 왕위를 유지한 인물은 누구인가?, \t\t True, \t\t None\n",
      "[May 05, 11:46:01] #> Output IDs: torch.Size([32]), tensor([     0,   9748,  24120,   1180,  13968, 211059,  83639,  76826,  78363,\n",
      "         57104,    993, 161732,    697, 116932, 114150,     32,      2,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1])\n",
      "[May 05, 11:46:01] #> Output Mask: torch.Size([32]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "[May 05, 11:46:01] #> XLMR DocTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
      "[May 05, 11:46:01] #> Input: $ \"Kangxi Emperor The Kangxi Emperor's reign of 61 years makes him the longest-reigning emperor in Chinese history (although his grandson, the Qianlong Emperor, had the longest period of \"\"de facto\"\" power) and one of the longest-reigning rulers in the world. However, since he ascended the throne at the age of seven, actual power was held for six years by four regents and his grandmother, the Grand Empress Dowager Xiaozhuang.\", \t\t None\n",
      "[May 05, 11:46:01] #> Output IDs: torch.Size([155]), tensor([     0,   9749,     44,  50245, 122809,  31678,     56,    748,    581,\n",
      "         30267,   5134,  31678,     56,    748,     25,      7,   1690,  38529,\n",
      "           111,  11716,   5369,  30482,   4049,     70,   4989,    525,      9,\n",
      "           107,    872,    592,      6,  88940,    748,     23,  76438,  32692,\n",
      "            15,    289, 197271,   1919,  69190,    191,      4,     70,  43954,\n",
      "            66,  10617,  31678,     56,    748,      4,   1902,     70,   4989,\n",
      "           525,  14922,    111,     44,     58,    112,  72563,     58,     58,\n",
      "         14537,     16,    136,   1632,    111,     70,   4989,    525,      9,\n",
      "           107,    872,    592,  79986,   4295,     23,     70,   8999,      5,\n",
      "         33306,      4,  16792,    764,  82800,    297,     70,      6,  42294,\n",
      "            86,     99,     70,  32070,    111,  59671,      4,   8561,  14537,\n",
      "           509,  34658,    100,  37195,   5369,    390,  22759, 119555,    933,\n",
      "           136,   1919,   9963,    432,   9319,      4,     70,  12801,   2775,\n",
      "         11856, 145058,  33730, 211423,    169,   1132,   1463,   1242,      2,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1])\n",
      "[May 05, 11:46:01] #> Output Mask: torch.Size([155]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "[May 05, 11:46:01] #>>>> colbert query ==\n",
      "[May 05, 11:46:01] #>>>>> input_ids: torch.Size([32]), tensor([     0,   9748,  24120,   1180,  13968, 211059,  83639,  76826,  78363,\n",
      "         57104,    993, 161732,    697, 116932, 114150,     32,      2,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/franzm/packages/minconda3/envs/ColBERT_oneqa_7/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/u/franzm/packages/minconda3/envs/ColBERT_oneqa_7/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
      "/u/franzm/packages/minconda3/envs/ColBERT_oneqa_7/lib/python3.9/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[May 05, 11:46:01] #>>>> before linear query ==\n",
      "[May 05, 11:46:01] #>>>>> Q: torch.Size([32, 768]), tensor([[ 0.1269,  0.0868,  0.0588,  ..., -0.0978,  0.0423,  0.0024],\n",
      "        [ 0.0237,  0.0695,  0.0303,  ..., -0.0671, -0.0818, -0.0653],\n",
      "        [ 0.0626,  0.0780,  0.0324,  ...,  0.0213, -0.0027,  0.1598],\n",
      "        ...,\n",
      "        [ 0.1314,  0.0969,  0.0671,  ..., -0.1059,  0.0561, -0.0338],\n",
      "        [ 0.0398,  0.0168,  0.0476,  ..., -0.0210, -0.0167, -0.0480],\n",
      "        [ 0.0871,  0.0832,  0.0208,  ..., -0.0546,  0.0003,  0.0225]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "[May 05, 11:46:01] #>>>>> self.linear query : Parameter containing:\n",
      "tensor([[-1.2613e-02, -3.6824e-03, -1.0773e-02,  ..., -1.3896e-02,\n",
      "          1.2244e-02, -8.2326e-03],\n",
      "        [-3.4988e-03,  3.9623e-03, -2.4905e-02,  ..., -2.6214e-02,\n",
      "          4.0837e-03, -8.0812e-03],\n",
      "        [-1.1158e-02, -2.1660e-05,  1.5019e-02,  ..., -1.1310e-02,\n",
      "         -9.3691e-03, -2.2202e-02],\n",
      "        ...,\n",
      "        [-1.0295e-02,  3.2577e-02,  3.3341e-03,  ...,  3.1876e-02,\n",
      "          3.6851e-03,  2.2995e-02],\n",
      "        [ 1.5563e-03,  2.2341e-03,  1.3362e-02,  ..., -2.8300e-02,\n",
      "          3.9382e-03,  1.1980e-02],\n",
      "        [-5.5801e-03,  3.1922e-02, -2.3421e-02,  ..., -1.1762e-04,\n",
      "          1.1104e-02, -9.7669e-03]], requires_grad=True)\n",
      "[May 05, 11:46:01] #>>>> colbert query ==\n",
      "[May 05, 11:46:01] #>>>>> Q: torch.Size([32, 128]), tensor([[-2.0807e-01, -2.8308e-02, -3.4714e-01,  ..., -1.5437e-01,\n",
      "         -1.1283e-01, -1.9922e-01],\n",
      "        [-5.8863e-02,  2.5084e-01, -4.4934e-01,  ..., -2.2064e-01,\n",
      "         -2.4872e-01, -6.0744e-02],\n",
      "        [ 1.5445e-03,  2.7442e-01, -4.6942e-01,  ..., -3.4489e-01,\n",
      "         -2.1474e-01, -1.7858e-02],\n",
      "        ...,\n",
      "        [-2.0095e-01, -3.4476e-02, -3.4331e-01,  ..., -1.5825e-01,\n",
      "         -1.2848e-01, -1.9769e-01],\n",
      "        [-1.4014e-01,  1.9043e-01, -4.8735e-01,  ..., -2.1481e-01,\n",
      "         -3.0219e-01, -1.2825e-01],\n",
      "        [-1.7410e-01, -2.4073e-04, -3.4931e-01,  ..., -1.7014e-01,\n",
      "         -1.2034e-01, -1.9180e-01]], grad_fn=<SelectBackward0>)\n",
      "[May 05, 11:46:01] #>>>> colbert doc ==\n",
      "[May 05, 11:46:01] #>>>>> input_ids: torch.Size([155]), tensor([     0,   9749,     44,  50245, 122809,  31678,     56,    748,    581,\n",
      "         30267,   5134,  31678,     56,    748,     25,      7,   1690,  38529,\n",
      "           111,  11716,   5369,  30482,   4049,     70,   4989,    525,      9,\n",
      "           107,    872,    592,      6,  88940,    748,     23,  76438,  32692,\n",
      "            15,    289, 197271,   1919,  69190,    191,      4,     70,  43954,\n",
      "            66,  10617,  31678,     56,    748,      4,   1902,     70,   4989,\n",
      "           525,  14922,    111,     44,     58,    112,  72563,     58,     58,\n",
      "         14537,     16,    136,   1632,    111,     70,   4989,    525,      9,\n",
      "           107,    872,    592,  79986,   4295,     23,     70,   8999,      5,\n",
      "         33306,      4,  16792,    764,  82800,    297,     70,      6,  42294,\n",
      "            86,     99,     70,  32070,    111,  59671,      4,   8561,  14537,\n",
      "           509,  34658,    100,  37195,   5369,    390,  22759, 119555,    933,\n",
      "           136,   1919,   9963,    432,   9319,      4,     70,  12801,   2775,\n",
      "         11856, 145058,  33730, 211423,    169,   1132,   1463,   1242,      2,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1])\n",
      "[May 05, 11:46:01] #>>>> before linear doc ==\n",
      "[May 05, 11:46:01] #>>>>> D: torch.Size([155, 768]), tensor([[ 0.0867,  0.1007,  0.0726,  ..., -0.0399,  0.0469, -0.0280],\n",
      "        [-0.0560,  0.0295, -0.0113,  ..., -0.0767, -0.0689, -0.0421],\n",
      "        [-0.0180,  0.0234, -0.0086,  ...,  0.0178,  0.0338,  0.0239],\n",
      "        ...,\n",
      "        [ 0.0670,  0.1110,  0.0430,  ..., -0.0891, -0.0012,  0.0213],\n",
      "        [ 0.0930,  0.1011,  0.0189,  ..., -0.1237,  0.0069, -0.0042],\n",
      "        [ 0.0750,  0.1035,  0.0231,  ..., -0.1795, -0.0286,  0.0083]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "[May 05, 11:46:01] #>>>>> self.linear doc : Parameter containing:\n",
      "tensor([[-1.2613e-02, -3.6824e-03, -1.0773e-02,  ..., -1.3896e-02,\n",
      "          1.2244e-02, -8.2326e-03],\n",
      "        [-3.4988e-03,  3.9623e-03, -2.4905e-02,  ..., -2.6214e-02,\n",
      "          4.0837e-03, -8.0812e-03],\n",
      "        [-1.1158e-02, -2.1660e-05,  1.5019e-02,  ..., -1.1310e-02,\n",
      "         -9.3691e-03, -2.2202e-02],\n",
      "        ...,\n",
      "        [-1.0295e-02,  3.2577e-02,  3.3341e-03,  ...,  3.1876e-02,\n",
      "          3.6851e-03,  2.2995e-02],\n",
      "        [ 1.5563e-03,  2.2341e-03,  1.3362e-02,  ..., -2.8300e-02,\n",
      "          3.9382e-03,  1.1980e-02],\n",
      "        [-5.5801e-03,  3.1922e-02, -2.3421e-02,  ..., -1.1762e-04,\n",
      "          1.1104e-02, -9.7669e-03]], requires_grad=True)\n",
      "[May 05, 11:46:01] #>>>> colbert doc ==\n",
      "[May 05, 11:46:01] #>>>>> D: torch.Size([155, 128]), tensor([[-0.1933, -0.0139, -0.3628,  ..., -0.1452, -0.1189, -0.1785],\n",
      "        [-0.0756,  0.2043, -0.4927,  ..., -0.1633, -0.2681, -0.0607],\n",
      "        [-0.0516,  0.2256, -0.5057,  ..., -0.2203, -0.2230, -0.0105],\n",
      "        ...,\n",
      "        [-0.1702,  0.0041, -0.3880,  ..., -0.1042, -0.1178, -0.1780],\n",
      "        [-0.1983, -0.0173, -0.3721,  ..., -0.1311, -0.0983, -0.1707],\n",
      "        [-0.1884,  0.0173, -0.3752,  ..., -0.1619, -0.1253, -0.1812]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "#>>>    -1.88 -2.39 \t\t|\t\t 0.5100000000000002\n",
      "[May 05, 11:46:04] 0 0.4700525104999542\n",
      "#>>>    -1.25 -2.81 \t\t|\t\t 1.56\n",
      "[May 05, 11:46:06] 1 0.4700525104999542\n",
      "#>>>    -0.71 -3.08 \t\t|\t\t 2.37\n",
      "[May 05, 11:46:08] 2 0.4700525104999542\n",
      "#> Saving a checkpoint to /tmp/tmpol5v7ufl/output_dir/test_training/none/2022-05/05/11.45.43/checkpoints/colbert.dnn.batch_3.model ..\n",
      "#> Saving a checkpoint to /tmp/tmpol5v7ufl/output_dir/test_training/none/2022-05/05/11.45.43/checkpoints/colbert-batch_3 ..\n",
      "[May 05, 11:46:15] name:/tmp/tmpol5v7ufl/output_dir/test_training/none/2022-05/05/11.45.43/checkpoints/colbert-LAST.dnn\n",
      "[May 05, 11:46:15] Make a sym link of /tmp/tmpol5v7ufl/output_dir/test_training/none/2022-05/05/11.45.43/checkpoints/colbert.dnn.batch_3.model to /tmp/tmpol5v7ufl/output_dir/test_training/none/2022-05/05/11.45.43/checkpoints/colbert-LAST.dnn\n",
      "[May 05, 11:46:15] #> Done with all triples!\n",
      "#> Saving a checkpoint to /tmp/tmpol5v7ufl/output_dir/test_training/none/2022-05/05/11.45.43/checkpoints/colbert ..\n"
     ]
    }
   ],
   "source": [
    "with Run().context(RunConfig(root=args_dict['root'], experiment=args_dict['experiment'], nranks=args_dict['nranks'], amp=args_dict['amp'])):\n",
    "    colBERTConfig = ColBERTConfig(**args_dict)\n",
    "    latest_model_fn = train(colBERTConfig, text_triples_fn, None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6d09e9",
   "metadata": {},
   "source": [
    "## Indexing\n",
    "Next, we will index a collection of documents, using model representaion from the previous step. \n",
    "The collection is a TSV file, containing each document's ID, title, and text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7191fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_fn = os.path.join(test_files_location, \"xorqa.train_ir_001pct_at_0_pct_collection_fornum.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e662a2ea",
   "metadata": {},
   "source": [
    "Here is an example document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3be63d29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The Kangxi Emperor's reign of 61 years makes him the longest-reigning emperor in Chinese history (although his grandson, the Qianlong Emperor, had the longest period of \"de facto\" power) and one of the longest-reigning rulers in the world. However, since he ascended the throne at the age of seven, actual power was held for six years by four regents and his grandmother, the Grand Empress Dowager Xiaozhuang.</td>\n",
       "      <td>Kangxi Emperor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pd.read_csv(collection_fn, sep='\\t', header=0, nrows=1)\n",
    "#data = pd.read_csv(collection_fn, sep='\\t', header=None, skiprows=3, nrows=1)\n",
    "display(HTML(data.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfadb75a",
   "metadata": {},
   "source": [
    "Here are the indexer arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "049c8272",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict = {\n",
    "                'root': os.path.join(output_dir,'test_indexing'),\n",
    "                'experiment': 'test_indexing',\n",
    "                'checkpoint': latest_model_fn,\n",
    "                'collection': collection_fn,\n",
    "                'index_root': os.path.join(output_dir, 'test_indexing', 'indexes'),\n",
    "                'index_name': 'index_name',\n",
    "                'doc_maxlen': 180,\n",
    "                'num_partitions_max': 2,\n",
    "                'kmeans_niters': 1,\n",
    "                'nway': 1,\n",
    "                'rank': 0,\n",
    "                'nranks': 1,\n",
    "                'amp': True\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96f1dbe",
   "metadata": {},
   "source": [
    "Here we run the indexer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4078d9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[May 05, 11:46:18] #> Creating directory /tmp/tmpol5v7ufl/output_dir/test_indexing/indexes/index_name \n",
      "\n",
      "\n",
      "{\n",
      "    \"nprobe\": 2,\n",
      "    \"ncandidates\": 8192,\n",
      "    \"index_path\": null,\n",
      "    \"nbits\": 1,\n",
      "    \"kmeans_niters\": 1,\n",
      "    \"num_partitions_max\": 2,\n",
      "    \"similarity\": \"cosine\",\n",
      "    \"bsize\": 32,\n",
      "    \"accumsteps\": 1,\n",
      "    \"lr\": 3e-6,\n",
      "    \"maxsteps\": 500000,\n",
      "    \"save_every\": null,\n",
      "    \"resume\": false,\n",
      "    \"resume_optimizer\": false,\n",
      "    \"warmup\": null,\n",
      "    \"warmup_bert\": null,\n",
      "    \"relu\": false,\n",
      "    \"nway\": 1,\n",
      "    \"use_ib_negatives\": false,\n",
      "    \"reranker\": false,\n",
      "    \"distillation_alpha\": 1.0,\n",
      "    \"ignore_scores\": false,\n",
      "    \"shuffle_every_epoch\": false,\n",
      "    \"save_steps\": 2000,\n",
      "    \"save_epochs\": -1,\n",
      "    \"epochs\": 10,\n",
      "    \"input_arguments\": {},\n",
      "    \"model_type\": \"bert-base-uncased\",\n",
      "    \"init_from_lm\": null,\n",
      "    \"local_models_repository\": null,\n",
      "    \"ranks_fn\": null,\n",
      "    \"topK\": 100,\n",
      "    \"student_teacher_temperature\": 1.0,\n",
      "    \"student_teacher_top_loss_weight\": 0.5,\n",
      "    \"teacher_model_type\": \"xlm-roberta-base\",\n",
      "    \"teacher_doc_maxlen\": 180,\n",
      "    \"distill_query_passage_separately\": false,\n",
      "    \"query_only\": false,\n",
      "    \"loss_function\": null,\n",
      "    \"query_weight\": 0.5,\n",
      "    \"query_maxlen\": 32,\n",
      "    \"attend_to_mask_tokens\": false,\n",
      "    \"interaction\": \"colbert\",\n",
      "    \"dim\": 128,\n",
      "    \"doc_maxlen\": 180,\n",
      "    \"mask_punctuation\": true,\n",
      "    \"checkpoint\": \"\\/tmp\\/tmpol5v7ufl\\/output_dir\\/test_training\\/none\\/2022-05\\/05\\/11.45.43\\/checkpoints\\/colbert\",\n",
      "    \"teacher_checkpoint\": null,\n",
      "    \"triples\": null,\n",
      "    \"teacher_triples\": null,\n",
      "    \"collection\": \"..\\/..\\/..\\/tests\\/resources\\/ir_dense\\/xorqa.train_ir_001pct_at_0_pct_collection_fornum.tsv\",\n",
      "    \"queries\": null,\n",
      "    \"index_name\": \"index_name\",\n",
      "    \"overwrite\": false,\n",
      "    \"root\": \"\\/tmp\\/tmpol5v7ufl\\/output_dir\\/test_indexing\",\n",
      "    \"experiment\": \"test_indexing\",\n",
      "    \"index_root\": \"\\/tmp\\/tmpol5v7ufl\\/output_dir\\/test_indexing\\/indexes\",\n",
      "    \"name\": \"2022-05\\/05\\/11.45.43\",\n",
      "    \"rank\": 0,\n",
      "    \"nranks\": 1,\n",
      "    \"amp\": true,\n",
      "    \"gpus\": 0\n",
      "}\n",
      "[May 05, 11:46:18] #> Loading collection...\n",
      "0M \n",
      "[May 05, 11:46:18] #>>>>> at ColBERT name (model type) : /tmp/tmpol5v7ufl/output_dir/test_training/none/2022-05/05/11.45.43/checkpoints/colbert\n",
      "[May 05, 11:46:18] #>>>>> at BaseColBERT name (model type) : /tmp/tmpol5v7ufl/output_dir/test_training/none/2022-05/05/11.45.43/checkpoints/colbert\n",
      "[May 05, 11:46:18] #> base_config.py load_from_checkpoint /tmp/tmpol5v7ufl/output_dir/test_training/none/2022-05/05/11.45.43/checkpoints/colbert\n",
      "[May 05, 11:46:18] #> base_config.py load_from_checkpoint /tmp/tmpol5v7ufl/output_dir/test_training/none/2022-05/05/11.45.43/checkpoints/colbert/artifact.metadata\n",
      "[May 05, 11:46:18] #> base_config.py from_path /tmp/tmpol5v7ufl/output_dir/test_training/none/2022-05/05/11.45.43/checkpoints/colbert/artifact.metadata\n",
      "[May 05, 11:46:18] #> base_config.py from_path args loaded! \n",
      "[May 05, 11:46:18] json file (get_colbert_from_pretrained): /tmp/tmpol5v7ufl/output_dir/test_training/none/2022-05/05/11.45.43/checkpoints/colbert/config.json\n",
      "[May 05, 11:46:18] factory model type: xlm-roberta-base\n",
      "[May 05, 11:46:27] json file (get_query_tokenizer): /tmp/tmpol5v7ufl/output_dir/test_training/none/2022-05/05/11.45.43/checkpoints/colbert/config.json\n",
      "[May 05, 11:46:27] get query model type: xlm-roberta-base\n",
      "[May 05, 11:46:28] json file (get_doc_tokenizer): /tmp/tmpol5v7ufl/output_dir/test_training/none/2022-05/05/11.45.43/checkpoints/colbert/config.json\n",
      "[May 05, 11:46:28] get doc model type: xlm-roberta-base\n",
      "[May 05, 11:46:30] [0] \t\t # of sampled PIDs = 7 \t sampled_pids[:3] = [3, 5, 0]\n",
      "[May 05, 11:46:30] [0] \t\t #> Encoding 7 passages..\n",
      "[May 05, 11:46:30] #> checkpoint, docFromText, Input: title | text, \t\t 32\n",
      "[May 05, 11:46:30] #> XLMR DocTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
      "[May 05, 11:46:30] #> Input: $ title | text, \t\t 32\n",
      "[May 05, 11:46:30] #> Output IDs: torch.Size([180]), tensor([    0,  9749, 44759,     6, 58745,  7986,     2,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1])\n",
      "[May 05, 11:46:30] #> Output Mask: torch.Size([180]), tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "[May 05, 11:46:30] #> checkpoint, docFromText, Output IDs: (tensor([[    0,  9749, 44759,  ...,     1,     1,     1],\n",
      "        [    0,  9749, 30267,  ...,     1,     1,     1],\n",
      "        [    0,  9749, 31678,  ...,     5,     2,     1],\n",
      "        ...,\n",
      "        [    0,  9749,  9098,  ...,     1,     1,     1],\n",
      "        [    0,  9749,   341,  ...,  4989,   525,     2],\n",
      "        [    0,  9749, 11617,  ...,     1,     1,     1]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]))\n",
      "[May 05, 11:46:30] #>>>> colbert doc ==\n",
      "[May 05, 11:46:30] #>>>>> input_ids: torch.Size([180]), tensor([    0,  9749, 44759,     6, 58745,  7986,     2,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/franzm/packages/minconda3/envs/ColBERT_oneqa_7/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
      "/u/franzm/packages/minconda3/envs/ColBERT_oneqa_7/lib/python3.9/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[May 05, 11:46:31] #>>>> before linear doc ==\n",
      "[May 05, 11:46:31] #>>>>> D: torch.Size([180, 768]), tensor([[ 7.1369e-02,  9.6666e-02,  5.4361e-02,  ..., -1.4448e-01,\n",
      "          5.4727e-02,  1.2758e-04],\n",
      "        [ 5.5677e-03,  2.6758e-02,  3.2364e-02,  ..., -1.3423e-01,\n",
      "          1.2431e-02,  1.1201e-01],\n",
      "        [-4.3298e-02,  9.1787e-02, -3.2852e-02,  ..., -2.9251e-01,\n",
      "         -2.6410e-02,  8.9396e-02],\n",
      "        ...,\n",
      "        [-5.3112e-02,  5.4709e-02,  4.0843e-03,  ..., -1.7059e-01,\n",
      "         -4.2763e-02,  2.6375e-03],\n",
      "        [-5.3112e-02,  5.4709e-02,  4.0843e-03,  ..., -1.7059e-01,\n",
      "         -4.2763e-02,  2.6375e-03],\n",
      "        [-5.3112e-02,  5.4709e-02,  4.0843e-03,  ..., -1.7059e-01,\n",
      "         -4.2763e-02,  2.6375e-03]])\n",
      "[May 05, 11:46:31] #>>>>> self.linear doc : Parameter containing:\n",
      "tensor([[-1.2620e-02, -3.6884e-03, -1.0780e-02,  ..., -1.3896e-02,\n",
      "          1.2237e-02, -8.2247e-03],\n",
      "        [-3.5069e-03,  3.9557e-03, -2.4896e-02,  ..., -2.6217e-02,\n",
      "          4.0822e-03, -8.0775e-03],\n",
      "        [-1.1149e-02, -1.4444e-05,  1.5010e-02,  ..., -1.1314e-02,\n",
      "         -9.3701e-03, -2.2194e-02],\n",
      "        ...,\n",
      "        [-1.0288e-02,  3.2584e-02,  3.3388e-03,  ...,  3.1875e-02,\n",
      "          3.6858e-03,  2.2995e-02],\n",
      "        [ 1.5650e-03,  2.2251e-03,  1.3353e-02,  ..., -2.8309e-02,\n",
      "          3.9365e-03,  1.1989e-02],\n",
      "        [-5.5845e-03,  3.1916e-02, -2.3427e-02,  ..., -1.1541e-04,\n",
      "          1.1096e-02, -9.7595e-03]], requires_grad=True)\n",
      "[May 05, 11:46:31] #>>>> colbert doc ==\n",
      "[May 05, 11:46:31] #>>>>> D: torch.Size([180, 128]), tensor([[-0.2274, -0.0232, -0.3379,  ..., -0.1237, -0.1311, -0.2028],\n",
      "        [-0.0830,  0.1467, -0.3716,  ..., -0.2201, -0.2307, -0.0847],\n",
      "        [-0.0835,  0.1630, -0.5002,  ..., -0.1637, -0.2573, -0.0865],\n",
      "        ...,\n",
      "        [-0.1618,  0.1807, -0.4258,  ..., -0.1398, -0.3287, -0.1716],\n",
      "        [-0.1618,  0.1807, -0.4258,  ..., -0.1398, -0.3287, -0.1716],\n",
      "        [-0.1618,  0.1807, -0.4258,  ..., -0.1398, -0.3287, -0.1716]])\n",
      "[May 05, 11:46:31] [0] \t\t avg_doclen_est = 174.2857208251953 \t len(local_sample) = 7\n",
      "[May 05, 11:46:31] >> num_partitions_multiplier = 8, self.num_partitions = 256\n",
      "[May 05, 11:46:31] >> num_partitions limited to: self.num_partitions = 2\n",
      "[May 05, 11:46:31] [0] \t\t Creaing 2 partitions.\n",
      "[May 05, 11:46:31] [0] \t\t *Estimated* 1,220 embeddings.\n",
      "[May 05, 11:46:31] [0] \t\t #> Saving the indexing plan to /tmp/tmpol5v7ufl/output_dir/test_indexing/indexes/index_name/plan.json ..\n",
      "Sampling a subset of 512 / 610 for training\n",
      "Clustering 512 points in 128D to 2 clusters, redo 1 times, 1 iterations\n",
      "  Preprocessing in 0.00 s\n",
      "[0.017, 0.019, 0.012, 0.013, 0.011, 0.021, 0.016, 0.014, 0.015, 0.012, 0.016, 0.014, 0.019, 0.012, 0.027, 0.019, 0.018, 0.012, 0.018, 0.011, 0.013, 0.02, 0.022, 0.014, 0.019, 0.015, 0.011, 0.017, 0.015, 0.017, 0.013, 0.014, 0.011, 0.019, 0.017, 0.019, 0.01, 0.016, 0.011, 0.018, 0.011, 0.014, 0.022, 0.024, 0.012, 0.011, 0.015, 0.009, 0.014, 0.023, 0.022, 0.018, 0.017, 0.014, 0.012, 0.025, 0.01, 0.015, 0.016, 0.012, 0.012, 0.01, 0.014, 0.011, 0.015, 0.011, 0.016, 0.015, 0.017, 0.015, 0.01, 0.016, 0.013, 0.016, 0.023, 0.011, 0.013, 0.013, 0.01, 0.01, 0.01, 0.016, 0.015, 0.019, 0.014, 0.018, 0.015, 0.02, 0.014, 0.013, 0.018, 0.013, 0.015, 0.026, 0.013, 0.013, 0.016, 0.019, 0.011, 0.01, 0.013, 0.016, 0.017, 0.014, 0.018, 0.02, 0.019, 0.016, 0.02, 0.019, 0.018, 0.02, 0.011, 0.016, 0.013, 0.016, 0.02, 0.016, 0.021, 0.017, 0.023, 0.013, 0.024, 0.011, 0.019, 0.014, 0.018, 0.017]\n",
      "[May 05, 11:46:31] #> Got bucket_cutoffs_quantiles = tensor([0.5000]) and bucket_weights_quantiles = tensor([0.2500, 0.7500])\n",
      "[May 05, 11:46:31] #> Got bucket_cutoffs = tensor([3.0518e-05]) and bucket_weights = tensor([-0.0127,  0.0117])\n",
      "[May 05, 11:46:31] avg_residual = 0.0156707763671875\n",
      "[May 05, 11:46:31] #> base_config.py from_path /tmp/tmpol5v7ufl/output_dir/test_indexing/indexes/index_name/metadata.json\n",
      "[May 05, 11:46:31] #> base_config.py from_path /tmp/tmpol5v7ufl/output_dir/test_indexing/indexes/index_name/plan.json\n",
      "[May 05, 11:46:31] #> base_config.py from_path args loaded! \n",
      "[May 05, 11:46:31] #> base_config.py from_path args replaced ! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[May 05, 11:46:31] [0] \t\t #> Encoding 7 passages..\n",
      "[May 05, 11:46:32] [0] \t\t #> Saving chunk 0: \t 7 passages and 1,220 embeddings. From #0 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[May 05, 11:46:32] offset: 0\n",
      "[May 05, 11:46:32] chunk codes size(0): 1220\n",
      "[May 05, 11:46:32] codes size(0): 1220\n",
      "[May 05, 11:46:32] codes size(): torch.Size([1220])\n",
      "[May 05, 11:46:32] >>>>partition.size(0): 2\n",
      "[May 05, 11:46:32] >>>>num_partition: 2\n",
      "[May 05, 11:46:32] [0] \t\t #> Saving the indexing metadata to /tmp/tmpol5v7ufl/output_dir/test_indexing/indexes/index_name/metadata.json ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with Run().context(RunConfig(root=args_dict['root'], experiment=args_dict['experiment'], nranks=args_dict['nranks'], amp=args_dict['amp'])):\n",
    "    colBERTConfig = ColBERTConfig(**args_dict)\n",
    "    create_directory(colBERTConfig.index_path_)\n",
    "    encode(colBERTConfig, collection_fn, None, None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b414c082",
   "metadata": {},
   "source": [
    "The resulting index files are in `output_dir/test_indexing/indexes/index_name/metadata.json`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfae0f75",
   "metadata": {},
   "source": [
    "## Search\n",
    "Next, we use the trained model and the index to search the collection, using queries from a TSV query file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9821799",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_fn = os.path.join(test_files_location, \"xorqa.train_ir_001pct_at_0_pct_queries_fornum.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40998e9e",
   "metadata": {},
   "source": [
    "Here are the search arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdb5b7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict = {\n",
    "                'root': output_dir,\n",
    "                'experiment': 'test_indexing' ,\n",
    "                'checkpoint': latest_model_fn,\n",
    "                'model_type': model_type,\n",
    "                'collection': collection_fn,\n",
    "                'index_root': output_dir,\n",
    "                'index_name': 'index_name',\n",
    "                'queries': queries_fn,\n",
    "                #'ranks_fn': ranks_fn,\n",
    "                'bsize': 1,\n",
    "                'topK': 1,\n",
    "                'nprobe': 1,\n",
    "                'nway': 1,\n",
    "                'rank': 0,\n",
    "                'nranks': 1,\n",
    "                'amp': True,\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95e619e",
   "metadata": {},
   "source": [
    "Here we initalize and run the searcher:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a23cb798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[May 05, 11:46:33] #> base_config.py from_path /tmp/tmpol5v7ufl/output_dir/test_indexing/indexes/index_name/metadata.json\n",
      "[May 05, 11:46:33] #> base_config.py from_path args loaded! \n",
      "[May 05, 11:46:33] #> base_config.py from_path args replaced ! \n",
      "[May 05, 11:46:33] #> base_config.py load_from_checkpoint /tmp/tmpol5v7ufl/output_dir/test_training/none/2022-05/05/11.45.43/checkpoints/colbert\n",
      "[May 05, 11:46:33] #> base_config.py load_from_checkpoint /tmp/tmpol5v7ufl/output_dir/test_training/none/2022-05/05/11.45.43/checkpoints/colbert/artifact.metadata\n",
      "[May 05, 11:46:33] #> base_config.py from_path /tmp/tmpol5v7ufl/output_dir/test_training/none/2022-05/05/11.45.43/checkpoints/colbert/artifact.metadata\n",
      "[May 05, 11:46:33] #> base_config.py from_path args loaded! \n",
      "[May 05, 11:46:33] #> Loading collection...\n",
      "0M \n",
      "[May 05, 11:46:33] #>>>>> at ColBERT name (model type) : /tmp/tmpol5v7ufl/output_dir/test_training/none/2022-05/05/11.45.43/checkpoints/colbert\n",
      "[May 05, 11:46:33] #>>>>> at BaseColBERT name (model type) : /tmp/tmpol5v7ufl/output_dir/test_training/none/2022-05/05/11.45.43/checkpoints/colbert\n",
      "[May 05, 11:46:33] #> base_config.py load_from_checkpoint /tmp/tmpol5v7ufl/output_dir/test_training/none/2022-05/05/11.45.43/checkpoints/colbert\n",
      "[May 05, 11:46:33] #> base_config.py load_from_checkpoint /tmp/tmpol5v7ufl/output_dir/test_training/none/2022-05/05/11.45.43/checkpoints/colbert/artifact.metadata\n",
      "[May 05, 11:46:33] #> base_config.py from_path /tmp/tmpol5v7ufl/output_dir/test_training/none/2022-05/05/11.45.43/checkpoints/colbert/artifact.metadata\n",
      "[May 05, 11:46:33] #> base_config.py from_path args loaded! \n",
      "[May 05, 11:46:33] json file (get_colbert_from_pretrained): /tmp/tmpol5v7ufl/output_dir/test_training/none/2022-05/05/11.45.43/checkpoints/colbert/config.json\n",
      "[May 05, 11:46:33] factory model type: xlm-roberta-base\n",
      "[May 05, 11:46:42] json file (get_query_tokenizer): /tmp/tmpol5v7ufl/output_dir/test_training/none/2022-05/05/11.45.43/checkpoints/colbert/config.json\n",
      "[May 05, 11:46:42] get query model type: xlm-roberta-base\n",
      "[May 05, 11:46:44] json file (get_doc_tokenizer): /tmp/tmpol5v7ufl/output_dir/test_training/none/2022-05/05/11.45.43/checkpoints/colbert/config.json\n",
      "[May 05, 11:46:44] get doc model type: xlm-roberta-base\n",
      "[May 05, 11:46:45] #> base_config.py from_path /tmp/tmpol5v7ufl/output_dir/test_indexing/indexes/index_name/metadata.json\n",
      "[May 05, 11:46:45] #> base_config.py from_path args loaded! \n",
      "[May 05, 11:46:45] #> base_config.py from_path args replaced ! \n",
      "[May 05, 11:46:45] #> Building the emb2pid mapping..\n",
      "[May 05, 11:46:45] len(self.emb2pid) = 1220\n",
      "[May 05, 11:46:45] #> Loading the queries from ../../../tests/resources/ir_dense/xorqa.train_ir_001pct_at_0_pct_queries_fornum.tsv ...\n",
      "[May 05, 11:46:45] #> Got 1 queries. All QIDs are unique.\n",
      "\n",
      "[May 05, 11:46:45] #> XMLR QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
      "[May 05, 11:46:45] #> Input: $ 중국에서 가장 오랜기간 왕위를 유지한 인물은 누구인가?, \t\t True, \t\t None\n",
      "[May 05, 11:46:45] #> Output IDs: torch.Size([32]), tensor([     0,   9748,  24120,   1180,  13968, 211059,  83639,  76826,  78363,\n",
      "         57104,    993, 161732,    697, 116932, 114150,     32,      2,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1])\n",
      "[May 05, 11:46:45] #> Output Mask: torch.Size([32]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "[May 05, 11:46:45] #>>>> colbert query ==\n",
      "[May 05, 11:46:45] #>>>>> input_ids: torch.Size([32]), tensor([     0,   9748,  24120,   1180,  13968, 211059,  83639,  76826,  78363,\n",
      "         57104,    993, 161732,    697, 116932, 114150,     32,      2,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1])\n",
      "[May 05, 11:46:46] #>>>> before linear query ==\n",
      "[May 05, 11:46:46] #>>>>> Q: torch.Size([32, 768]), tensor([[ 0.1822,  0.1034,  0.0644,  ..., -0.1299,  0.0443, -0.0150],\n",
      "        [ 0.0746,  0.0515,  0.0609,  ..., -0.0692, -0.1220, -0.0613],\n",
      "        [ 0.1168,  0.0506,  0.0132,  ...,  0.0023, -0.0073,  0.2034],\n",
      "        ...,\n",
      "        [ 0.1733,  0.0921, -0.0351,  ..., -0.2894, -0.0817,  0.0497],\n",
      "        [ 0.1733,  0.0921, -0.0351,  ..., -0.2894, -0.0817,  0.0497],\n",
      "        [ 0.1733,  0.0921, -0.0351,  ..., -0.2894, -0.0817,  0.0497]])\n",
      "[May 05, 11:46:46] #>>>>> self.linear query : Parameter containing:\n",
      "tensor([[-1.2620e-02, -3.6884e-03, -1.0780e-02,  ..., -1.3896e-02,\n",
      "          1.2237e-02, -8.2247e-03],\n",
      "        [-3.5069e-03,  3.9557e-03, -2.4896e-02,  ..., -2.6217e-02,\n",
      "          4.0822e-03, -8.0775e-03],\n",
      "        [-1.1149e-02, -1.4444e-05,  1.5010e-02,  ..., -1.1314e-02,\n",
      "         -9.3701e-03, -2.2194e-02],\n",
      "        ...,\n",
      "        [-1.0288e-02,  3.2584e-02,  3.3388e-03,  ...,  3.1875e-02,\n",
      "          3.6858e-03,  2.2995e-02],\n",
      "        [ 1.5650e-03,  2.2251e-03,  1.3353e-02,  ..., -2.8309e-02,\n",
      "          3.9365e-03,  1.1989e-02],\n",
      "        [-5.5845e-03,  3.1916e-02, -2.3427e-02,  ..., -1.1541e-04,\n",
      "          1.1096e-02, -9.7595e-03]], requires_grad=True)\n",
      "[May 05, 11:46:46] #>>>> colbert query ==\n",
      "[May 05, 11:46:46] #>>>>> Q: torch.Size([32, 128]), tensor([[-0.2540, -0.0565, -0.3365,  ..., -0.1182, -0.0839, -0.2157],\n",
      "        [-0.0348,  0.2773, -0.4799,  ..., -0.2676, -0.2649, -0.0370],\n",
      "        [ 0.0164,  0.3203, -0.4733,  ..., -0.3585, -0.2623, -0.0090],\n",
      "        ...,\n",
      "        [-0.1896, -0.0439, -0.3298,  ..., -0.1296, -0.0436, -0.2189],\n",
      "        [-0.1896, -0.0439, -0.3298,  ..., -0.1296, -0.0436, -0.2189],\n",
      "        [-0.1896, -0.0439, -0.3298,  ..., -0.1296, -0.0436, -0.2189]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.05it/s]\n"
     ]
    }
   ],
   "source": [
    "with Run().context(RunConfig(root=args_dict['root'], experiment=args_dict['experiment'], nranks=args_dict['nranks'], amp=args_dict['amp'])):\n",
    "    colBERTConfig = ColBERTConfig(**args_dict)\n",
    "    searcher = Searcher(args_dict['index_name'], checkpoint=args_dict['checkpoint'], collection=args_dict['collection'], config=colBERTConfig)\n",
    "    rankings = searcher.search_all(args_dict['queries'], args_dict['topK'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ceac32",
   "metadata": {},
   "source": [
    "Here is the search result for our query [query_id, document_id, rank, score]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd8e1b51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-7239279093922981232, 1, 1, 29.515625)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rankings.flat_ranking[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a888747c",
   "metadata": {},
   "source": [
    "Here is the text of the query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6027f452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-7239279093922981232\t중국에서 가장 오랜기간 왕위를 유지한 인물은 누구인가?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(queries_fn, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        if str(rankings.flat_ranking[0][0]) == line.split()[0]:\n",
    "            print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf0a038",
   "metadata": {},
   "source": [
    "English translation: `Who maintained the throne for the longest time in China?`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1408c62",
   "metadata": {},
   "source": [
    "Here is the top retrieved document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89133f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t\"The Kangxi Emperor's reign of 61 years makes him the longest-reigning emperor in Chinese history (although his grandson, the Qianlong Emperor, had the longest period of \"\"de facto\"\" power) and one of the longest-reigning rulers in the world. However, since he ascended the throne at the age of seven, actual power was held for six years by four regents and his grandmother, the Grand Empress Dowager Xiaozhuang.\"\tKangxi Emperor\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(collection_fn, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        if str(rankings.flat_ranking[0][1]) == line.split()[0]:\n",
    "            print(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
