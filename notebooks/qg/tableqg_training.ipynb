{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "\n",
    "We start by setting some parameters to configure the process.  Note that depending on the GPU being used you may need to tune the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path=\"t5-small\"\n",
    "modality=\"table\"\n",
    "dataset_name=\"wikisql\"\n",
    "max_len=200\n",
    "target_max_len=40\n",
    "output_dir=\"models/qg/trials\"\n",
    "learning_rate=0.0001\n",
    "num_train_epochs=1\n",
    "per_device_train_batch_size=8\n",
    "per_device_eval_batch_size=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Loading the Model\n",
    "\n",
    "Here we load the model based on the model_name parameter set above.  We use a QG model for modality=table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    DataCollator,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from primeqa.qg.processors.data_loader import QGDataLoader\n",
    "import torch\n",
    "from dataclasses import dataclass,field\n",
    "from primeqa.qg.models.qg_model import QGModel\n",
    "from primeqa.qg.trainers.qg_trainer import QGTrainer\n",
    "from typing import Optional, List, Dict\n",
    "from primeqa.qg.trainers.qg_trainer_utils import T2TDataCollator, ModelArguments, DataTrainingArguments, QGTrainingArguments, InferenceArguments\n",
    "from examples.qg.run_qg import TrainingArguments\n",
    "\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "\n",
    "seed=42\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True,\n",
    "    do_eval=False,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    evaluation_strategy='no',\n",
    "    learning_rate=learning_rate,\n",
    "    prediction_loss_only=True,\n",
    "    remove_unused_columns=False,\n",
    "    seed=seed\n",
    "    )\n",
    "\n",
    "set_seed(training_args.seed)\n",
    "\n",
    "model_args = ModelArguments(\n",
    "        model_name_or_path=model_name_or_path,\n",
    "        modality=modality\n",
    "    )\n",
    "\n",
    "data_args = DataTrainingArguments(\n",
    "    dataset_name = dataset_name,\n",
    "    max_len = max_len,\n",
    "    target_max_len = target_max_len\n",
    "    )\n",
    "\n",
    "qg_model = QGModel(model_args.model_name_or_path, modality=model_args.modality)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "\n",
    "Here we load the Wikisql dataset using Huggingface's datasets library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset wiki_sql (/u/jaydesen/.cache/huggingface/datasets/wiki_sql/default/0.1.0/7037bfe6a42b1ca2b6ac3ccacba5253b1825d31379e9cc626fc79a620977252d)\n",
      "100%|██████████| 56355/56355 [00:24<00:00, 2286.49it/s]\n",
      "Parameter 'function'=<function QGDataLoader.convert_to_features at 0x7f0ab892ed08> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "682683fe44c841aea3d07fb8b9b8f75a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/dccstor/cssblr/jaydeep/1qa_env/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "Using custom data configuration default\n",
      "Reusing dataset wiki_sql (/u/jaydesen/.cache/huggingface/datasets/wiki_sql/default/0.1.0/7037bfe6a42b1ca2b6ac3ccacba5253b1825d31379e9cc626fc79a620977252d)\n",
      "100%|██████████| 8421/8421 [00:03<00:00, 2393.96it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db9c154d9cab4f65b25b36e51d6cad56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56355\n",
      "{'input_ids': tensor([ 1738, 32100,  2507,     7, 32100, 12892, 22031, 32101,  4081, 32101,\n",
      "          180,  9744,   566,     3,  6727, 13733, 24933,   188, 32102,   150,\n",
      "        22031,    30,   750,   939, 32103,  1015,    87,    17, 21301, 10972,\n",
      "        32104,  5027,    87,  1549,  9232,  3243, 32104, 12439, 32104, 12892,\n",
      "        22031, 32104, 12892,   939, 32104,  2507,     7,     1,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'target_ids': tensor([8779,  140,  125,    8, 3358,   33,   21, 1013, 2051,    1,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0]), 'target_attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}\n"
     ]
    }
   ],
   "source": [
    "qgdl = QGDataLoader(\n",
    "    tokenizer=qg_model.tokenizer,\n",
    "    dataset_name=data_args.dataset_name,\n",
    "    input_max_len=data_args.max_len,\n",
    "    target_max_len=data_args.target_max_len\n",
    "    )\n",
    "\n",
    "train_dataset = qgdl.create(\"train\")\n",
    "\n",
    "valid_dataset = qgdl.create(\"validation\")\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(train_dataset[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train using QGTrainer\n",
    "Here we create a QG trainer with the training arguments defined above and use it to train on Wikisql training data (or any custom data following the same format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dccstor/cssblr/jaydeep/1qa_env/lib/python3.7/site-packages/transformers/trainer.py:1135: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
      "  FutureWarning,\n",
      "/dccstor/cssblr/jaydeep/1qa_env/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 56355\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 7045\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1782' max='7045' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1782/7045 34:25 < 1:41:48, 0.86 it/s, Epoch 0.25/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.148400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.628900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.489600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to models/qg/trials/checkpoint-500\n",
      "Configuration saved in models/qg/trials/checkpoint-500/config.json\n",
      "Model weights saved in models/qg/trials/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in models/qg/trials/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in models/qg/trials/checkpoint-500/special_tokens_map.json\n",
      "Copy vocab file to models/qg/trials/checkpoint-500/spiece.model\n",
      "Saving model checkpoint to models/qg/trials/checkpoint-1000\n",
      "Configuration saved in models/qg/trials/checkpoint-1000/config.json\n",
      "Model weights saved in models/qg/trials/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in models/qg/trials/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in models/qg/trials/checkpoint-1000/special_tokens_map.json\n",
      "Copy vocab file to models/qg/trials/checkpoint-1000/spiece.model\n",
      "Saving model checkpoint to models/qg/trials/checkpoint-1500\n",
      "Configuration saved in models/qg/trials/checkpoint-1500/config.json\n",
      "Model weights saved in models/qg/trials/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in models/qg/trials/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in models/qg/trials/checkpoint-1500/special_tokens_map.json\n",
      "Copy vocab file to models/qg/trials/checkpoint-1500/spiece.model\n"
     ]
    }
   ],
   "source": [
    "trainer = QGTrainer(\n",
    "    model=qg_model.model,\n",
    "    tokenizer = qg_model.tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    valid_dataset=valid_dataset,\n",
    "    data_collator=T2TDataCollator()\n",
    "    )\n",
    "\n",
    "trainer.train(\n",
    "        model_path=model_args.model_name_or_path if os.path.isdir(model_args.model_name_or_path) else None\n",
    "        )\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2c35b992e6c7aefc6892dbea5982d2f0b243183ae5e95337e08b7ede6fdab7cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
