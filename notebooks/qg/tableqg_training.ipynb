{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table Question generation: WikiSQL dataset\n",
    "In this notebook, we will see how to fine-tune and evaluate a question generation model on WikiSQL dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "We start by setting some parameters to configure the process.  Note that depending on the GPU being used you may need to tune the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path=\"t5-small\"\n",
    "modality=\"table\"\n",
    "dataset_name=\"wikisql\"\n",
    "max_len=200\n",
    "target_max_len=40\n",
    "output_dir=\"../../models/qg/wikisql_nb\"\n",
    "learning_rate=0.0001\n",
    "num_train_epochs=2\n",
    "per_device_train_batch_size=8\n",
    "per_device_eval_batch_size=32\n",
    "evaluation_strategy='epoch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=learning_rate,\n",
    "    prediction_loss_only=True,\n",
    "    remove_unused_columns=False,\n",
    "    )\n",
    "training_args.predict_with_generate=True\n",
    "training_args.remove_unused_columns = False\n",
    "training_args.prediction_loss_only = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WikiSQL data\n",
    "Here we load one instance of WikiSQL and visualize it. This part of the code is not needed to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset wiki_sql (/dccstor/cmv/.cache/wiki_sql/default/0.1.0/7037bfe6a42b1ca2b6ac3ccacba5253b1825d31379e9cc626fc79a620977252d)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table:\n",
      " +-----------------------+-------------------------------+------------------------+--------------------+------------------------+\n",
      "| Aircraft              | Description                   | Max Gross Weight       | Total disk area    | Max disk Loading       |\n",
      "+=======================+===============================+========================+====================+========================+\n",
      "| Robinson R-22         | Light utility helicopter      | 1,370 lb (635 kg)      | 497 ft² (46.2 m²)  | 2.6 lb/ft² (14 kg/m²)  |\n",
      "+-----------------------+-------------------------------+------------------------+--------------------+------------------------+\n",
      "| Bell 206B3 JetRanger  | Turboshaft utility helicopter | 3,200 lb (1,451 kg)    | 872 ft² (81.1 m²)  | 3.7 lb/ft² (18 kg/m²)  |\n",
      "+-----------------------+-------------------------------+------------------------+--------------------+------------------------+\n",
      "| CH-47D Chinook        | Tandem rotor helicopter       | 50,000 lb (22,680 kg)  | 5,655 ft² (526 m²) | 8.8 lb/ft² (43 kg/m²)  |\n",
      "+-----------------------+-------------------------------+------------------------+--------------------+------------------------+\n",
      "| Mil Mi-26             | Heavy-lift helicopter         | 123,500 lb (56,000 kg) | 8,495 ft² (789 m²) | 14.5 lb/ft² (71 kg/m²) |\n",
      "+-----------------------+-------------------------------+------------------------+--------------------+------------------------+\n",
      "| CH-53E Super Stallion | Heavy-lift helicopter         | 73,500 lb (33,300 kg)  | 4,900 ft² (460 m²) | 15 lb/ft² (72 kg/m²)   |\n",
      "+-----------------------+-------------------------------+------------------------+--------------------+------------------------+\n",
      "Question =  What is the max gross weight of the Robinson R-22?\n",
      "SQL =  SELECT Max Gross Weight FROM table WHERE Aircraft = Robinson R-22\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from tabulate import tabulate\n",
    "\n",
    "def print_wikisql_instance(train_instance):\n",
    "    table = train_instance['table']\n",
    "    print('Table:\\n',tabulate(table['rows'], headers=table['header'], tablefmt='grid'))\n",
    "\n",
    "    print('Question = ',train_instance['question'])\n",
    "    print('SQL = ', train_instance['sql']['human_readable'])\n",
    "\n",
    "train_instance = load_dataset('wikisql', split='train[11:12]')[0]\n",
    "print_wikisql_instance(train_instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SQL gets converted to a string format which goes as input to generator to generate question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset wiki_sql (/dccstor/cmv/.cache/wiki_sql/default/0.1.0/7037bfe6a42b1ca2b6ac3ccacba5253b1825d31379e9cc626fc79a620977252d)\n",
      "100%|██████████| 1/1 [00:00<00:00, 43.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question =  What is the max gross weight of the Robinson R-22?\n",
      "\n",
      "Input to generator =  select <<sep>> Max Gross Weight <<sep>> Aircraft <<cond>> equal <<cond>> Robinson R-22 <<answer>> 1,370 lb (635 kg) <<header>> Aircraft <<hsep>> Description <<hsep>> Max Gross Weight <<hsep>> Total disk area <<hsep>> Max disk Loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from primeqa.qg.processors.table_qg.wikisql_processor import WikiSqlDataset\n",
    "\n",
    "data = WikiSqlDataset()\n",
    "processed_data = data.preprocess_data_for_qg('train[11:12]')\n",
    "print('Question = ', processed_data['question'][0])\n",
    "print('\\nInput to generator = ', processed_data['input'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Loading the Model\n",
    "\n",
    "Here we load the model based on the model_name and modality parameter set above. For WikiSQL we keep modality='table'. Other option is modality='passage'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from primeqa.qg.models.qg_model import QGModel\n",
    "\n",
    "qg_model = QGModel(model_name_or_path, modality=modality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "\n",
    "Here we load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset wiki_sql (/dccstor/cmv/.cache/wiki_sql/default/0.1.0/7037bfe6a42b1ca2b6ac3ccacba5253b1825d31379e9cc626fc79a620977252d)\n",
      "100%|██████████| 100/100 [00:00<00:00, 1183.50it/s]\n",
      "Parameter 'function'=<function QGDataLoader.convert_to_features at 0x7f9fb6415a70> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "900f3e7c48cb411890e26dfe86e53c04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/dccstor/cmv/saneem/nlqTable/irl_git/oneqa-env/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "Using custom data configuration default\n",
      "Reusing dataset wiki_sql (/dccstor/cmv/.cache/wiki_sql/default/0.1.0/7037bfe6a42b1ca2b6ac3ccacba5253b1825d31379e9cc626fc79a620977252d)\n",
      "100%|██████████| 50/50 [00:00<00:00, 1191.83it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bd755e77450456082cb954dd6d25aa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from primeqa.qg.processors.data_loader import QGDataLoader\n",
    "\n",
    "qgdl = QGDataLoader(\n",
    "    tokenizer=qg_model.tokenizer,\n",
    "    dataset_name=dataset_name,\n",
    "    input_max_len=max_len,\n",
    "    target_max_len=target_max_len\n",
    "    )\n",
    "\n",
    "train_dataset = qgdl.create(\"train[:100]\")\n",
    "valid_dataset = qgdl.create(\"validation[:50]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train using QGTrainer\n",
    "Here we create a QG trainer with the training arguments defined above and use it to train on Wikisql training data (or any custom data following the same format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from primeqa.qg.trainers.qg_trainer import QGTrainer\n",
    "from primeqa.qg.metrics.generation_metrics import rouge_metrics\n",
    "from primeqa.qg.utils.data_collator import T2TDataCollator\n",
    "import os\n",
    "\n",
    "compute_metrics = rouge_metrics(qg_model.tokenizer)\n",
    "\n",
    "trainer = QGTrainer(\n",
    "    model=qg_model.model,\n",
    "    tokenizer = qg_model.tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    valid_dataset=valid_dataset,\n",
    "    data_collator=T2TDataCollator(),\n",
    "    compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "train_results = trainer.train()\n",
    "trainer.save_model()\n",
    "print(train_results.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Here we evaluate the trained model on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = trainer.evaluate()\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f2837b60cedc613a72d719d2d261dedab01e8683bba6b8605ad579171c0f5b25"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2c35b992e6c7aefc6892dbea5982d2f0b243183ae5e95337e08b7ede6fdab7cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
