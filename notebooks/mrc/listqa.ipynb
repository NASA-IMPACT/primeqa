{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ListQA\n",
    "\n",
    "In this notebook, we will see how to evaluate a model on questions that have lists as answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies\n",
    "\n",
    "If not already done, make sure to install PrimeQA with `notebooks` extras before getting started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# If you want CUDA 11 uncomment and run this (for CUDA 10 or CPU you can ignore this line).\n",
    "#! pip install 'torch~=1.11.0' --extra-index-url https://download.pytorch.org/whl/cu113\n",
    "\n",
    "# Uncomment to install PrimeQA from source (pypi package pending).\n",
    "# The path should be the project root (e.g. '.' below).\n",
    "#! pip install .[notebooks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NQ List Subset\n",
    "\n",
    "Create train and dev subsets from NQ that have lists as answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading NQ data...\n",
      "INFO:root:/dccstor/srosent1/projects/mlqa/data/NQ/train/nq-train-08.jsonl.gz\n",
      "100%|██████████| 2000/2000 [00:41<00:00, 47.92it/s]\n",
      "INFO:root:Done\n",
      "INFO:root:Converting to tydi\n",
      "100%|██████████| 2000/2000 [00:18<00:00, 105.41it/s]\n",
      "INFO:root:data size: 15\n",
      "INFO:root:/dccstor/srosent1/projects/mlqa/data/NQ/train/nq-train-07.jsonl.gz\n",
      "100%|██████████| 2000/2000 [00:32<00:00, 61.66it/s]\n",
      "INFO:root:Done\n",
      "INFO:root:Converting to tydi\n",
      "100%|██████████| 2000/2000 [00:25<00:00, 79.38it/s] \n",
      "INFO:root:data size: 18\n",
      "INFO:root:/dccstor/srosent1/projects/mlqa/data/NQ/train/nq-train-02.jsonl.gz\n",
      "100%|██████████| 2000/2000 [00:33<00:00, 59.88it/s]\n",
      "INFO:root:Done\n",
      "INFO:root:Converting to tydi\n",
      "100%|██████████| 2000/2000 [00:25<00:00, 79.43it/s] \n",
      "INFO:root:data size: 12\n",
      "INFO:root:/dccstor/srosent1/projects/mlqa/data/NQ/train/nq-train-01.jsonl.gz\n",
      "100%|██████████| 2000/2000 [00:35<00:00, 56.40it/s]\n",
      "INFO:root:Done\n",
      "INFO:root:Converting to tydi\n",
      "100%|██████████| 2000/2000 [00:22<00:00, 87.97it/s] \n",
      "INFO:root:data size: 15\n",
      "INFO:root:/dccstor/srosent1/projects/mlqa/data/NQ/train/nq-train-06.jsonl.gz\n",
      "100%|██████████| 2000/2000 [00:38<00:00, 51.52it/s]\n",
      "INFO:root:Done\n",
      "INFO:root:Converting to tydi\n",
      "100%|██████████| 2000/2000 [00:22<00:00, 88.41it/s] \n",
      "INFO:root:data size: 10\n",
      "INFO:root:/dccstor/srosent1/projects/mlqa/data/NQ/train/nq-train-09.jsonl.gz\n",
      "100%|██████████| 2000/2000 [00:35<00:00, 56.87it/s]\n",
      "INFO:root:Done\n",
      "INFO:root:Converting to tydi\n",
      "100%|██████████| 2000/2000 [00:20<00:00, 96.32it/s] \n",
      "INFO:root:data size: 10\n",
      "INFO:root:/dccstor/srosent1/projects/mlqa/data/NQ/train/nq-train-00.jsonl.gz\n",
      "100%|██████████| 2000/2000 [00:33<00:00, 59.47it/s]\n",
      "INFO:root:Done\n",
      "INFO:root:Converting to tydi\n",
      "100%|██████████| 2000/2000 [00:21<00:00, 93.21it/s] \n",
      "INFO:root:data size: 15\n",
      "INFO:root:/dccstor/srosent1/projects/mlqa/data/NQ/train/nq-train-05.jsonl.gz\n",
      "100%|██████████| 2000/2000 [00:31<00:00, 62.93it/s]\n",
      "INFO:root:Done\n",
      "INFO:root:Converting to tydi\n",
      "100%|██████████| 2000/2000 [00:18<00:00, 105.39it/s]\n",
      "INFO:root:data size: 10\n",
      "INFO:root:/dccstor/srosent1/projects/mlqa/data/NQ/train/nq-train-03.jsonl.gz\n",
      "100%|██████████| 2000/2000 [00:35<00:00, 56.74it/s]\n",
      "INFO:root:Done\n",
      "INFO:root:Converting to tydi\n",
      "100%|██████████| 2000/2000 [00:19<00:00, 101.07it/s]\n",
      "INFO:root:data size: 9\n",
      "INFO:root:/dccstor/srosent1/projects/mlqa/data/NQ/train/nq-train-04.jsonl.gz\n",
      "100%|██████████| 2000/2000 [00:31<00:00, 62.59it/s]\n",
      "INFO:root:Done\n",
      "INFO:root:Converting to tydi\n",
      "100%|██████████| 2000/2000 [00:22<00:00, 89.72it/s] \n",
      "INFO:root:data size: 10\n",
      "INFO:root:count: 124\n",
      "INFO:root:average answer length: 1703.6935483870968\n",
      "INFO:root:average list length: 16.95967741935484\n",
      "INFO:root:Loading NQ data...\n",
      "INFO:root:/dccstor/srosent1/projects/mlqa/data/NQ/dev/nq-dev-01.jsonl.gz\n",
      "100%|██████████| 1000/1000 [00:15<00:00, 63.61it/s]\n",
      "INFO:root:Done\n",
      "INFO:root:Converting to tydi\n",
      "100%|██████████| 1000/1000 [00:10<00:00, 97.19it/s]\n",
      "INFO:root:data size: 26\n",
      "INFO:root:/dccstor/srosent1/projects/mlqa/data/NQ/dev/nq-dev-00.jsonl.gz\n",
      "100%|██████████| 1000/1000 [00:17<00:00, 56.80it/s]\n",
      "INFO:root:Done\n",
      "INFO:root:Converting to tydi\n",
      "100%|██████████| 1000/1000 [00:09<00:00, 108.31it/s]\n",
      "INFO:root:data size: 21\n",
      "INFO:root:count: 83\n",
      "INFO:root:average answer length: 837.6144578313254\n",
      "INFO:root:average list length: 10.397590361445783\n"
     ]
    }
   ],
   "source": [
    "from examples.listqa.list_nq2tydi import ListNQSubset\n",
    "\n",
    "input_train = \"/dccstor/srosent1/projects/mlqa/data/NQ/train/nq-train-0*.jsonl.gz\"\n",
    "input_dev = \"/dccstor/srosent1/projects/mlqa/data/NQ/dev/nq-dev-*jsonl.gz\"\n",
    "output_data_dir = \"/dccstor/srosent2/primeqa/data/nq/\"\n",
    "\n",
    "# num_lines is how many lines to read from each file. Use -1 to load full dataset. We only load a subset here for illustration purposes \n",
    "processor = ListNQSubset()\n",
    "processor.process(input_train, output_data_dir + \"/nq-train-lists.jsonl\", num_lines=5000)\n",
    "processor.process(input_dev, output_data_dir + \"/nq-dev-lists.jsonl\", num_lines=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "\n",
    "We start by setting some parameters to configure the process.  Note that depending on the GPU being used you may need to tune the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This needs to be filled in.\n",
    "output_dir = '/dccstor/srosent2/primeqa/experiments/run_nq'        # Save the results here.  Will overwrite if directory already exists.\n",
    "\n",
    "# Optional parameters (feel free to leave as default).\n",
    "model_name = 'roberta-base'  # Set this to select the LM.  We use a fine-tuned xlm-roberta model for list answers.\n",
    "cache_dir = None                 # Set this if you have a cache directory for transformers.  Alternatively set the HF_HOME env var.\n",
    "train_batch_size = 16             # Set this to change the number of features per batch during training.\n",
    "eval_batch_size = 16              # Set this to change the number of features per batch during evaluation.\n",
    "gradient_accumulation_steps = 8  # Set this to effectively increase training batch size.\n",
    "max_train_samples = 500          # Set this to use a subset of the training data (or None for all).\n",
    "max_eval_samples = 10            # Set this to use a subset of the evaluation data (or None for all).\n",
    "num_train_epochs = 1             # Set this to change the number of training epochs.\n",
    "fp16 = False                     # Set this to true to enable fp16 (hardware support required).\n",
    "num_examples_to_show = 5        # Set this to change the number of random train examples (and their features) to show.\n",
    "max_seq_length=512              # have large sequence length to accomodate lists which can be long (we don't want to split on lists)\n",
    "max_answer_length=1000          # have large answer length to accomodate lists which can be long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from transformers.trainer_utils import set_seed\n",
    "\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=False,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=train_batch_size,\n",
    "    per_device_eval_batch_size=eval_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    evaluation_strategy='no',\n",
    "    learning_rate=5e-05,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.1,\n",
    "    save_steps=50000,\n",
    "    fp16=fp16,\n",
    "    seed=seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Model\n",
    "\n",
    "Here we load the model and tokenizer based on the model_name parameter set above.  We use a model with an extractive QA task head which we will later fine-tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ExtractiveQAHead:Loading dropout value 0.1 from config attribute 'hidden_dropout_prob'\n",
      "Some weights of XLMRobertaModelForDownstreamTasks were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['task_heads.qa_head.qa_outputs.bias', 'task_heads.qa_head.classifier.dense.bias', 'task_heads.qa_head.qa_outputs.weight', 'task_heads.qa_head.classifier.dense.weight', 'task_heads.qa_head.classifier.out_proj.weight', 'task_heads.qa_head.classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:XLMRobertaModelForDownstreamTasks:Setting task head for first time to 'None'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XLMRobertaModelForDownstreamTasks(\n",
      "  (roberta): XLMRobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): RobertaPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (lm_head): RobertaLMHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (decoder): Linear(in_features=768, out_features=250002, bias=True)\n",
      "  )\n",
      "  (task_heads): ModuleDict(\n",
      "    (qa_head): ExtractiveQAHead(\n",
      "      (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
      "      (classifier): RobertaClassificationHead(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (out_proj): Linear(in_features=768, out_features=5, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer\n",
    "from primeqa.mrc.models.heads.extractive import EXTRACTIVE_HEAD\n",
    "from primeqa.mrc.models.task_model import ModelForDownstreamTasks\n",
    "\n",
    "from primeqa.mrc.trainers.mrc import MRCTrainer\n",
    "\n",
    "task_heads = EXTRACTIVE_HEAD\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=cache_dir,\n",
    "    use_fast=True,\n",
    "    config=config,\n",
    ")\n",
    "model = ModelForDownstreamTasks.from_config(\n",
    "    config,\n",
    "    model_name,\n",
    "    task_heads=task_heads,\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "model.set_task_head(next(iter(task_heads)))\n",
    "\n",
    "print(model)  # Examine the model structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "\n",
    "Here we load the NQ dataset from disk in TyDi Google format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration default-17b552305cac9100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /dccstor/srosent2/primeqa/experiments/run_nq/json/default-17b552305cac9100/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a2407bacea947578f771c668bb019b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8768acae8474bd7bdea6a20b55c4dcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /dccstor/srosent2/primeqa/experiments/run_nq/json/default-17b552305cac9100/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf0da156a63b4433a86ff8da46e222e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 5 eval examples.\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "raw_dataset = datasets.load_dataset('json', \n",
    "        data_files={'train':[output_data_dir+\"/nq-train-lists.jsonl\"],'dev':[output_data_dir+\"/nq-dev-lists.jsonl\"]},\n",
    "        cache_dir=training_args.output_dir)\n",
    "train_examples = raw_dataset['train']\n",
    "max_train_samples = max_train_samples\n",
    "if max_train_samples is not None:\n",
    "    # We will select sample from whole data if argument is specified\n",
    "    train_examples = train_examples.select(range(max_train_samples))\n",
    "eval_examples = raw_dataset['dev']\n",
    "max_eval_samples = max_eval_samples\n",
    "if max_eval_samples is not None:\n",
    "    # We will select sample from whole data if argument is specified\n",
    "    eval_examples = eval_examples.select(range(max_eval_samples))\n",
    "\n",
    "print(f\"Using {eval_examples.num_rows} eval examples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Here we preprocess the data to create features which can be given to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:TyDiQAGooglePreprocessor:TyDiQAGooglePreprocessor only supports single context multiple passages -- enabling\n",
      "WARNING:datasets.fingerprint:Parameter 'function'=<function TyDiQAGooglePreprocessor._rename_examples at 0x7f0ed0e569d0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d98322fb805e4e51ae9ddaeff34ae83f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "243073fd0bde41b18d56daf8795391d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd07570466244bad898be00a87e5e632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1726e213a50456d9a36809bc786ca07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on train dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing produced 382 train features from 100 examples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d33f201f217147ceb10f4bf8b9a611d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "badeed2334f14f9f8a5bc0dcd90b0427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4b02ec412aa4af5ae0deba36afb4bbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab98ce6d6a34458bb9f0e6a437582f5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on eval dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing produced 65 eval features from 5 examples.\n"
     ]
    }
   ],
   "source": [
    "from primeqa.mrc.processors.preprocessors.tydiqa_google import TyDiQAGooglePreprocessor\n",
    "\n",
    "preprocessor = TyDiQAGooglePreprocessor(\n",
    "    stride=256,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_len=max_seq_length,\n",
    ")\n",
    "\n",
    "# Training Feature Creation\n",
    "with training_args.main_process_first(desc=\"train dataset map pre-processing\"):\n",
    "    train_examples, train_dataset = preprocessor.process_train(train_examples)\n",
    "\n",
    "print(f\"Preprocessing produced {train_dataset.num_rows} train features from {train_examples.num_rows} examples.\")\n",
    "\n",
    "# Validation Feature Creation\n",
    "with training_args.main_process_first(desc=\"validation dataset map pre-processing\"):\n",
    "    eval_examples, eval_dataset = preprocessor.process_eval(eval_examples)\n",
    "\n",
    "print(f\"Preprocessing produced {eval_dataset.num_rows} eval features from {eval_examples.num_rows} examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import ClassLabel, Sequence\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Based on https://github.com/huggingface/notebooks/blob/main/examples/question_answering.ipynb\n",
    "def show_elements(dataset):\n",
    "    df = pd.DataFrame(dataset)\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4209e51ec2040f4a59b93290e7f350c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'document_plaintext': Value(dtype='string', id=None), 'document_title': Value(dtype='string', id=None), 'document_url': Value(dtype='string', id=None), 'example_id': Value(dtype='int64', id=None), 'language': Value(dtype='string', id=None), 'question': Value(dtype='string', id=None), 'passage_candidates': {'end_positions': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'start_positions': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}, 'target': {'end_positions': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'passage_indices': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'start_positions': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'yes_no_answer': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}, 'context': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16839567e62d464e96f9806541725491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_title</th>\n",
       "      <th>document_url</th>\n",
       "      <th>example_id</th>\n",
       "      <th>language</th>\n",
       "      <th>question</th>\n",
       "      <th>target</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mamma Mia! Here We Go Again</td>\n",
       "      <td>https://en.wikipedia.org//w/index.php?title=Mamma_Mia!_Here_We_Go_Again&amp;amp;oldid=846364736</td>\n",
       "      <td>2796511963863747460</td>\n",
       "      <td>english</td>\n",
       "      <td>mama mia 2 here we go again cast</td>\n",
       "      <td>{'end_positions': [3719], 'passage_indices': [27], 'start_positions': [2233], 'yes_no_answer': ['NONE']}</td>\n",
       "      <td>Mamma Mia ! Here We Go Again - Wikipedia Mamma Mia ! Here We Go Again Mamma Mia ! Here We Go Again Theatrical release poster Directed by Ol Parker Produced by * Judy Craymer * Gary Goetzman Screenplay by Ol Parker Story by * Catherine Johnson * Richard Curtis * Ol Parker Based on Mamma Mia ! by Catherine Johnson Starring * Christine Baranski * Pierce Brosnan * Dominic Cooper * Colin Firth * Andy García * Lily James * Amanda Seyfried * Stellan Skarsgård * Julie Walters * Cher * Meryl Streep Mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gunsmoke</td>\n",
       "      <td>https://en.wikipedia.org//w/index.php?title=Gunsmoke&amp;amp;oldid=836953511</td>\n",
       "      <td>-1267223525065420052</td>\n",
       "      <td>english</td>\n",
       "      <td>how many episodes of gunsmoke was burt reynolds on</td>\n",
       "      <td>{'end_positions': [45852], 'passage_indices': [179], 'start_positions': [44202], 'yes_no_answer': ['NONE']}</td>\n",
       "      <td>Gunsmoke - wikipedia Gunsmoke Jump to : navigation , search This article is about the radio and television series . For other uses , see Gun Smoke . James Arness as Matt Dillon in the television version of Gunsmoke ( 1956 ) Gunsmoke is an American radio and television Western drama series created by director Norman Macdonnell and writer John Meston . The stories take place in and around Dodge City , Kansas , during the settlement of the American West . The central character is lawman Marshal ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Fab Four (tribute)</td>\n",
       "      <td>https://en.wikipedia.org//w/index.php?title=The_Fab_Four_(tribute)&amp;amp;oldid=854972994</td>\n",
       "      <td>684722901270187660</td>\n",
       "      <td>english</td>\n",
       "      <td>what are the names of the fab four</td>\n",
       "      <td>{'end_positions': [2880], 'passage_indices': [15], 'start_positions': [2133], 'yes_no_answer': ['NONE']}</td>\n",
       "      <td>The Fab Four ( tribute ) - wikipedia The Fab Four ( tribute ) The Fab Four Background information Origin California Genres Rock and roll Beatles tribute Years active 1997 - Present Labels Delta Ent . ( Laserlight ) , New World Digital Website www.TheFabFour.com Members Ron McNeil Gilbert Bonilla Tyson Kelly Ardy Sarraf Neil Candelora Michael Amador Gavin Pring Rolo Sandoval Joe Bologna Luis Renteria Erik Fidel The Fab Four is a California - based tribute band paying homage to The Beatles . Fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rise of the Guardians</td>\n",
       "      <td>https://en.wikipedia.org//w/index.php?title=Rise_of_the_Guardians&amp;amp;oldid=826399004</td>\n",
       "      <td>-5871963713943154664</td>\n",
       "      <td>english</td>\n",
       "      <td>who plays voices in rise of the guardians</td>\n",
       "      <td>{'end_positions': [6904], 'passage_indices': [27], 'start_positions': [5187], 'yes_no_answer': ['NONE']}</td>\n",
       "      <td>Rise of the Guardians - Wikipedia Rise of the Guardians Jump to : navigation , search This article is about the film . For the video game , see Rise of the Guardians : The Video Game . Rise of the Guardians Theatrical release poster Directed by Peter Ramsey Produced by Christina Steinberg Nancy Bernstein Screenplay by David Lindsay - Abaire Based on The Guardians of Childhood by William Joyce Starring Chris Pine Alec Baldwin Hugh Jackman Isla Fisher Jude Law Music by Alexandre Desplat Edited ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Who Stole the Cookie from the Cookie Jar?</td>\n",
       "      <td>https://en.wikipedia.org//w/index.php?title=Who_Stole_the_Cookie_from_the_Cookie_Jar%3F&amp;amp;oldid=834546477</td>\n",
       "      <td>-4735931497873009375</td>\n",
       "      <td>english</td>\n",
       "      <td>who ate the cookies from the cookie jar song lyrics</td>\n",
       "      <td>{'end_positions': [1858], 'passage_indices': [4], 'start_positions': [1553], 'yes_no_answer': ['NONE']}</td>\n",
       "      <td>Who stole the cookie from the cookie jar ? - Wikipedia Who stole the cookie from the cookie jar ? `` Cookie Jar Song '' redirects here . For the Gym Class Heroes song , see Cookie Jar ( song ) . `` Who Stole the Cookie from the Cookie Jar ? '' or the Cookie Jar Song is a sing along and game of children 's music . The song is an infinite - loop motif , where each verse directly feeds into the next . The game begins with the children sitting or standing , arranged in an inward - facing circle ....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def trim_document(example, max_len=500):\n",
    "    example['context'] = example['context'][0]\n",
    "    doc_len = len(example['context'])\n",
    "    if doc_len > max_len:\n",
    "        example['context'] = f\"{example['context'][:max_len - 3]}...\"        \n",
    "    return example\n",
    "\n",
    "from datasets.features.features import Value\n",
    "train_examples = train_examples.cast_column('example_id',Value(dtype=\"int64\",id=None))\n",
    "print(train_examples.features)\n",
    "\n",
    "random_idxs = random.sample(range(len(train_examples)), num_examples_to_show)\n",
    "random_examples = train_examples.select(random_idxs).remove_columns(['document_plaintext', 'passage_candidates'])\n",
    "random_examples = random_examples.map(trim_document)\n",
    "\n",
    "show_elements(random_examples)  # Show random train examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb5fe067dd8c42b3a50f90a643caeaa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>example_id</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>example_idx</th>\n",
       "      <th>start_positions</th>\n",
       "      <th>end_positions</th>\n",
       "      <th>target_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2796511963863747460</td>\n",
       "      <td>[0, 8840, 8132, 116, 3688, 642, 738, 13438, 37702, 2, 2, 24668, 111, 70, 5701, 9351, 6, 5, 581, 1346, 6057, 7, 6035, 8613, 24714, 4517, 6, 4, 141499, 14631, 6, 4, 119497, 74583, 84285, 6, 4, 161055, 22411, 32673, 6, 4, 64672, 80490, 7, 6, 4, 47231, 329, 126449, 4458, 6, 4, 166132, 35526, 927, 6, 4, 124172, 19, 104964, 7, 33653, 6, 4, 120725, 59797, 6, 4, 136, 70535, 6, 5, 1650, 83, 80889, 71, 47, 186, 121447, 23, 23924, 21629, 136, 70, 17274, 98, 20414, 387, 6, 4, 267, 390, 53900, 134896, 6, 4, 1492, 5369, 47, ...]</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2796511963863747460</td>\n",
       "      <td>[0, 8840, 8132, 116, 3688, 642, 738, 13438, 37702, 2, 2, 7, 15, 47231, 329, 126449, 4458, 6, 4, 124172, 19, 104964, 7, 33653, 6, 4, 136, 166132, 35526, 927, 1388, 6, 4, 136, 165249, 10, 76849, 6, 4, 1620, 272, 538, 756, 98, 604, 10002, 6, 4, 15490, 10, 42732, 47, 17997, 604, 4210, 678, 142, 241957, 19922, 1295, 22008, 2412, 1902, 959, 77049, 71, 707, 84751, 47, 1957, 152, 604, 9963, 432, 9319, 6, 4, 158189, 53257, 12667, 6, 5, 32301, 15, 27211, 1388, 661, 119497, 74583, 84285, 237, 145906, 53257, 12667, 6, 4, 140716, 242, 7, 496, ...]</td>\n",
       "      <td>67</td>\n",
       "      <td>86</td>\n",
       "      <td>491</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2796511963863747460</td>\n",
       "      <td>[0, 8840, 8132, 116, 3688, 642, 738, 13438, 37702, 2, 2, 459, 6496, 42179, 661, 20984, 11, 151466, 237, 44389, 12015, 478, 661, 47231, 329, 126449, 4458, 237, 3362, 3980, 266, 1436, 583, 6, 4, 145906, 242, 7, 29954, 1021, 9319, 136, 7722, 67373, 6, 4, 142, 120552, 20, 15672, 107653, 6, 4, 158189, 242, 7, 775, 20, 23, 20, 27165, 6, 4, 136, 71390, 47, 140716, 661, 169707, 6921, 34271, 237, 44389, 3362, 661, 166132, 35526, 927, 237, 20904, 138276, 6, 4, 145906, 242, 7, 7722, 67373, 136, 10, 56101, 150065, 6, 5, 661, 198952, 68761, 1679, 237, 44389, ...]</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from primeqa.mrc.data_models.target_type import TargetType\n",
    "\n",
    "random_train_dataset = train_dataset.filter(lambda feature: feature['example_idx'] in random_idxs[:1]).remove_columns(['attention_mask', 'offset_mapping'])\n",
    "show_elements(random_train_dataset) # Show random train features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import attrgetter\n",
    "from transformers import DataCollatorWithPadding\n",
    "from primeqa.mrc.data_models.eval_prediction_with_processing import EvalPredictionWithProcessing\n",
    "from primeqa.mrc.metrics.tydi_f1.tydi_f1 import TyDiF1\n",
    "from primeqa.mrc.processors.postprocessors.extractive import ExtractivePostProcessor\n",
    "from primeqa.mrc.processors.postprocessors.scorers import SupportedSpanScorers\n",
    "\n",
    "# If using mixed precision we pad for efficient hardware acceleration\n",
    "using_mixed_precision = any(attrgetter('fp16', 'bf16')(training_args))\n",
    "data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=64 if using_mixed_precision else None)\n",
    "\n",
    "# noinspection PyProtectedMember\n",
    "postprocessor = ExtractivePostProcessor(\n",
    "    k=3,\n",
    "    n_best_size=20,\n",
    "    max_answer_length=max_answer_length,\n",
    "    scorer_type=SupportedSpanScorers.WEIGHTED_SUM_TARGET_TYPE_AND_SCORE_DIFF,\n",
    "    single_context_multiple_passages=preprocessor._single_context_multiple_passages,\n",
    ")\n",
    "\n",
    "def compute_metrics(p: EvalPredictionWithProcessing):\n",
    "    return TyDiF1().compute(predictions=p.processed_predictions, references=p.label_ids, passage_non_null_threshold=1, span_non_null_threshold=1)\n",
    "\n",
    "trainer = MRCTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=None,\n",
    "    eval_dataset=eval_dataset if training_args.do_eval else None,\n",
    "    eval_examples=eval_examples if training_args.do_eval else None,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    post_process_function=postprocessor.process_references_and_predictions,  # see QATrainer in Huggingface\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Here we evaluate the model on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:primeqa.mrc.trainers.mrc:The following columns in the evaluation set  don't have a corresponding argument in `XLMRobertaModelForDownstreamTasks.forward` and have been ignored: offset_mapping, example_id, example_idx.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 65\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 15.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passage & english & \\fpr{0.0}{0.0}{0.0}\n",
      "Minimal Answer & english & \\fpr{0.0}{0.0}{0.0}\n",
      "********************\n",
      "english\n",
      "Language: english (5)\n",
      "********************\n",
      "PASSAGE ANSWER R@P TABLE:\n",
      "Optimal threshold: 0.0\n",
      " F1     /  P      /  R\n",
      "  0.00% /   0.00% /   0.00%\n",
      "R@P=0.5: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.75: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.9: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "********************\n",
      "MINIMAL ANSWER R@P TABLE:\n",
      "Optimal threshold: 0.0\n",
      " F1     /  P      /  R\n",
      "  0.00% /   0.00% /   0.00%\n",
      "R@P=0.5: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.75: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.9: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "Total # examples in gold: 5, # ex. in pred: 5 (including english)\n",
      "*** Macro Over 0 Languages, excluding English **\n",
      "Passage F1:0.000 P:0.000 R:0.000000\n",
      "\\fpr{0.0}{0.0}{0.0}\n",
      "Minimal F1:0.000 P:0.000 R:0.000000\n",
      "\\fpr{0.0}{0.0}{0.0}\n",
      "*** / Aggregate Scores ****\n",
      "{\"avg_passage_f1\": 0, \"avg_passage_recall\": 0, \"avg_passage_precision\": 0, \"avg_minimal_f1\": 0, \"avg_minimal_recall\": 0, \"avg_minimal_precision\": 0}\n",
      "***** eval metrics *****\n",
      "  eval_avg_minimal_f1        = 0\n",
      "  eval_avg_minimal_precision = 0\n",
      "  eval_avg_minimal_recall    = 0\n",
      "  eval_avg_passage_f1        = 0\n",
      "  eval_avg_passage_precision = 0\n",
      "  eval_avg_passage_recall    = 0\n",
      "  eval_samples               = 5\n"
     ]
    }
   ],
   "source": [
    "max_eval_samples = max_eval_samples or len(eval_dataset)\n",
    "\n",
    "metrics = trainer.evaluate()\n",
    "metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n",
    "\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions\n",
    "\n",
    "Here we examine the model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'-8498955431733322253': [{'cls_score': -0.4986354410648346,\n",
      "                           'confidence_score': 0.3383668723153968,\n",
      "                           'end_index': 120,\n",
      "                           'end_logit': 0.12701758742332458,\n",
      "                           'end_stdev': 0.0,\n",
      "                           'example_id': '-8498955431733322253',\n",
      "                           'normalized_span_answer_score': 0.3383668723153968,\n",
      "                           'passage_index': -1,\n",
      "                           'query_passage_similarity': 0.0,\n",
      "                           'span_answer': {'end_position': 1302,\n",
      "                                           'start_position': 1299},\n",
      "                           'span_answer_score': 0.40101583674550056,\n",
      "                           'span_answer_text': 'tym',\n",
      "                           'start_index': 120,\n",
      "                           'start_logit': 0.23672683537006378,\n",
      "                           'start_stdev': 0.0,\n",
      "                           'target_type_logits': [0.22214165329933167,\n",
      "                                                  -0.06034819036722183,\n",
      "                                                  0.1492932140827179,\n",
      "                                                  0.07679904997348785,\n",
      "                                                  -0.41610413789749146],\n",
      "                           'yes_no_answer': 0},\n",
      "                          {'cls_score': -0.4986354410648346,\n",
      "                           'confidence_score': 0.3329820624581045,\n",
      "                           'end_index': 120,\n",
      "                           'end_logit': 0.12701758742332458,\n",
      "                           'end_stdev': 0.0,\n",
      "                           'example_id': '-8498955431733322253',\n",
      "                           'normalized_span_answer_score': 0.3329820624581045,\n",
      "                           'passage_index': -1,\n",
      "                           'query_passage_similarity': 0.0,\n",
      "                           'span_answer': {'end_position': 1302,\n",
      "                                           'start_position': 1114},\n",
      "                           'span_answer_score': 0.384973730891943,\n",
      "                           'span_answer_text': 'tymology and pronunciation * 2 '\n",
      "                                               'Steps of mechanism * 3 Types * '\n",
      "                                               '4 In emergency medicine * 5 '\n",
      "                                               'Disorders * 6 History of '\n",
      "                                               'artificial hemostasis * 7 '\n",
      "                                               'Research * 8 References * 9 '\n",
      "                                               'External links Etym',\n",
      "                           'start_index': 74,\n",
      "                           'start_logit': 0.2046426236629486,\n",
      "                           'start_stdev': 0.0,\n",
      "                           'target_type_logits': [0.22214165329933167,\n",
      "                                                  -0.06034819036722183,\n",
      "                                                  0.1492932140827179,\n",
      "                                                  0.07679904997348785,\n",
      "                                                  -0.41610413789749146],\n",
      "                           'yes_no_answer': 0},\n",
      "                          {'cls_score': -0.4986354410648346,\n",
      "                           'confidence_score': 0.3286510652264986,\n",
      "                           'end_index': 120,\n",
      "                           'end_logit': 0.12701758742332458,\n",
      "                           'end_stdev': 0.0,\n",
      "                           'example_id': '-8498955431733322253',\n",
      "                           'normalized_span_answer_score': 0.3286510652264986,\n",
      "                           'passage_index': -1,\n",
      "                           'query_passage_similarity': 0.0,\n",
      "                           'span_answer': {'end_position': 1302,\n",
      "                                           'start_position': 1117},\n",
      "                           'span_answer_score': 0.37188170477747917,\n",
      "                           'span_answer_text': 'ology and pronunciation * 2 '\n",
      "                                               'Steps of mechanism * 3 Types * '\n",
      "                                               '4 In emergency medicine * 5 '\n",
      "                                               'Disorders * 6 History of '\n",
      "                                               'artificial hemostasis * 7 '\n",
      "                                               'Research * 8 References * 9 '\n",
      "                                               'External links Etym',\n",
      "                           'start_index': 75,\n",
      "                           'start_logit': 0.178458571434021,\n",
      "                           'start_stdev': 0.0,\n",
      "                           'target_type_logits': [0.22214165329933167,\n",
      "                                                  -0.06034819036722183,\n",
      "                                                  0.1492932140827179,\n",
      "                                                  0.07679904997348785,\n",
      "                                                  -0.41610413789749146],\n",
      "                           'yes_no_answer': 0}],\n",
      " '4776979995967131111': [{'cls_score': -0.45647601783275604,\n",
      "                          'confidence_score': 0.33736197546545926,\n",
      "                          'end_index': 293,\n",
      "                          'end_logit': -0.02140205353498459,\n",
      "                          'end_stdev': 0.0,\n",
      "                          'example_id': '4776979995967131111',\n",
      "                          'normalized_span_answer_score': 0.33736197546545926,\n",
      "                          'passage_index': -1,\n",
      "                          'query_passage_similarity': 0.0,\n",
      "                          'span_answer': {'end_position': 1117,\n",
      "                                          'start_position': 997},\n",
      "                          'span_answer_score': 0.28396982327103615,\n",
      "                          'span_answer_text': \"oldid=829959267 '' Categories : \"\n",
      "                                              '* Disambiguation pages Hidden '\n",
      "                                              'categories : * Disambiguation '\n",
      "                                              'pages with short description',\n",
      "                          'start_index': 265,\n",
      "                          'start_logit': 0.186680868268013,\n",
      "                          'start_stdev': 0.0,\n",
      "                          'target_type_logits': [0.2175857424736023,\n",
      "                                                 -0.05381518602371216,\n",
      "                                                 0.1345100700855255,\n",
      "                                                 0.07556615769863129,\n",
      "                                                 -0.4134191572666168],\n",
      "                          'yes_no_answer': 0},\n",
      "                         {'cls_score': -0.45647601783275604,\n",
      "                          'confidence_score': 0.3319139148338753,\n",
      "                          'end_index': 293,\n",
      "                          'end_logit': -0.02140205353498459,\n",
      "                          'end_stdev': 0.0,\n",
      "                          'example_id': '4776979995967131111',\n",
      "                          'normalized_span_answer_score': 0.3319139148338753,\n",
      "                          'passage_index': -1,\n",
      "                          'query_passage_similarity': 0.0,\n",
      "                          'span_answer': {'end_position': 1117,\n",
      "                                          'start_position': 930},\n",
      "                          'span_answer_score': 0.2676890008151531,\n",
      "                          'span_answer_text': 'wikipedia.org/w/index.php?title=Good_things_come_to_those_who_wait&oldid=829959267 '\n",
      "                                              \"'' Categories : * \"\n",
      "                                              'Disambiguation pages Hidden '\n",
      "                                              'categories : * Disambiguation '\n",
      "                                              'pages with short description',\n",
      "                          'start_index': 235,\n",
      "                          'start_logit': 0.15411922335624695,\n",
      "                          'start_stdev': 0.0,\n",
      "                          'target_type_logits': [0.2175857424736023,\n",
      "                                                 -0.05381518602371216,\n",
      "                                                 0.1345100700855255,\n",
      "                                                 0.07556615769863129,\n",
      "                                                 -0.4134191572666168],\n",
      "                          'yes_no_answer': 0},\n",
      "                         {'cls_score': -0.45647601783275604,\n",
      "                          'confidence_score': 0.3307241097006655,\n",
      "                          'end_index': 293,\n",
      "                          'end_logit': -0.02140205353498459,\n",
      "                          'end_stdev': 0.0,\n",
      "                          'example_id': '4776979995967131111',\n",
      "                          'normalized_span_answer_score': 0.3307241097006655,\n",
      "                          'passage_index': -1,\n",
      "                          'query_passage_similarity': 0.0,\n",
      "                          'span_answer': {'end_position': 1117,\n",
      "                                          'start_position': 946},\n",
      "                          'span_answer_score': 0.2640978805720806,\n",
      "                          'span_answer_text': 'index.php?title=Good_things_come_to_those_who_wait&oldid=829959267 '\n",
      "                                              \"'' Categories : * \"\n",
      "                                              'Disambiguation pages Hidden '\n",
      "                                              'categories : * Disambiguation '\n",
      "                                              'pages with short description',\n",
      "                          'start_index': 241,\n",
      "                          'start_logit': 0.14693698287010193,\n",
      "                          'start_stdev': 0.0,\n",
      "                          'target_type_logits': [0.2175857424736023,\n",
      "                                                 -0.05381518602371216,\n",
      "                                                 0.1345100700855255,\n",
      "                                                 0.07556615769863129,\n",
      "                                                 -0.4134191572666168],\n",
      "                          'yes_no_answer': 0}],\n",
      " '609499642367269804': [{'cls_score': -0.488163560628891,\n",
      "                         'confidence_score': 0.33548746496569715,\n",
      "                         'end_index': 412,\n",
      "                         'end_logit': 0.07513580471277237,\n",
      "                         'end_stdev': 0.0,\n",
      "                         'example_id': '609499642367269804',\n",
      "                         'normalized_span_answer_score': 0.33548746496569715,\n",
      "                         'passage_index': -1,\n",
      "                         'query_passage_similarity': 0.0,\n",
      "                         'span_answer': {'end_position': 4616,\n",
      "                                         'start_position': 4207},\n",
      "                         'span_answer_score': 0.3758069984614849,\n",
      "                         'span_answer_text': 'rinoco river llanos or savannas '\n",
      "                                             ', from where this climate takes '\n",
      "                                             'its name . Sometimes As is used '\n",
      "                                             'in place of Aw if the dry season '\n",
      "                                             'occurs during the time of higher '\n",
      "                                             'sun and longer days often due to '\n",
      "                                             'a rain shadow effect that cuts '\n",
      "                                             'off summer precipitation in a '\n",
      "                                             'tropical area . This is the case '\n",
      "                                             'in parts of Hawaii , East Africa '\n",
      "                                             '( Mombasa , Kenya , Somalia ) , '\n",
      "                                             'Sri Lanka ( Trincomalee ) and '\n",
      "                                             'coastal regions of Northe',\n",
      "                         'start_index': 309,\n",
      "                         'start_logit': 0.22942207753658295,\n",
      "                         'start_stdev': 0.0,\n",
      "                         'target_type_logits': [0.20924583077430725,\n",
      "                                                -0.04110744595527649,\n",
      "                                                0.1519128531217575,\n",
      "                                                0.0554320365190506,\n",
      "                                                -0.409682959318161],\n",
      "                         'yes_no_answer': 0},\n",
      "                        {'cls_score': -0.4813050925731659,\n",
      "                         'confidence_score': 0.33362755885877926,\n",
      "                         'end_index': 428,\n",
      "                         'end_logit': 0.007257286459207535,\n",
      "                         'end_stdev': 0.0,\n",
      "                         'example_id': '609499642367269804',\n",
      "                         'normalized_span_answer_score': 0.33362755885877926,\n",
      "                         'passage_index': -1,\n",
      "                         'query_passage_similarity': 0.0,\n",
      "                         'span_answer': {'end_position': 9438,\n",
      "                                         'start_position': 9033},\n",
      "                         'span_answer_score': 0.37024768255650997,\n",
      "                         'span_answer_text': 'hrvatski / српскохрватски * ไทย '\n",
      "                                             '* Українська * Tiếng Việt * 中文 '\n",
      "                                             'Edit links * This page was last '\n",
      "                                             'edited on 6 January 2018 , at 20 '\n",
      "                                             ': 54 . * Text is available under '\n",
      "                                             'the Creative Commons Attribution '\n",
      "                                             '- ShareAlike License ; '\n",
      "                                             'additional terms may apply . By '\n",
      "                                             'using this site , you agree to '\n",
      "                                             'the Terms of Use and Privacy '\n",
      "                                             'Policy . Wikipedia ® is a '\n",
      "                                             'registered trademark of the '\n",
      "                                             'Wikimedia Foundation , Inc. , a '\n",
      "                                             'non-profit',\n",
      "                         'start_index': 335,\n",
      "                         'start_logit': 0.29965290427207947,\n",
      "                         'start_stdev': 0.0,\n",
      "                         'target_type_logits': [0.21106965839862823,\n",
      "                                                -0.04771991819143295,\n",
      "                                                0.13717544078826904,\n",
      "                                                0.06626227498054504,\n",
      "                                                -0.4175073206424713],\n",
      "                         'yes_no_answer': 0},\n",
      "                        {'cls_score': -0.488163560628891,\n",
      "                         'confidence_score': 0.3308849761755236,\n",
      "                         'end_index': 335,\n",
      "                         'end_logit': 0.04750823974609375,\n",
      "                         'end_stdev': 0.0,\n",
      "                         'example_id': '609499642367269804',\n",
      "                         'normalized_span_answer_score': 0.3308849761755236,\n",
      "                         'passage_index': -1,\n",
      "                         'query_passage_similarity': 0.0,\n",
      "                         'span_answer': {'end_position': 4311,\n",
      "                                         'start_position': 4207},\n",
      "                         'span_answer_score': 0.3619932159781456,\n",
      "                         'span_answer_text': 'rinoco river llanos or savannas '\n",
      "                                             ', from where this climate takes '\n",
      "                                             'its name . Sometimes As is used '\n",
      "                                             'in place',\n",
      "                         'start_index': 309,\n",
      "                         'start_logit': 0.22942207753658295,\n",
      "                         'start_stdev': 0.0,\n",
      "                         'target_type_logits': [0.20924583077430725,\n",
      "                                                -0.04110744595527649,\n",
      "                                                0.1519128531217575,\n",
      "                                                0.0554320365190506,\n",
      "                                                -0.409682959318161],\n",
      "                         'yes_no_answer': 0}],\n",
      " '682243000439863277': [{'cls_score': -0.4857536405324936,\n",
      "                         'confidence_score': 0.33431284485949786,\n",
      "                         'end_index': 284,\n",
      "                         'end_logit': 0.14462396502494812,\n",
      "                         'end_stdev': 0.0,\n",
      "                         'example_id': '682243000439863277',\n",
      "                         'normalized_span_answer_score': 0.33431284485949786,\n",
      "                         'passage_index': -1,\n",
      "                         'query_passage_similarity': 0.0,\n",
      "                         'span_answer': {'end_position': 2112,\n",
      "                                         'start_position': 1772},\n",
      "                         'span_answer_score': 0.36965235881507397,\n",
      "                         'span_answer_text': 'References * 6 External links '\n",
      "                                             'Plot ( edit ) As America '\n",
      "                                             'recovers from the Civil War , '\n",
      "                                             'Paul Cable ( Tom Selleck ) '\n",
      "                                             'returns home to Texas after '\n",
      "                                             'being away from his family for '\n",
      "                                             'years while fighting for the '\n",
      "                                             'Confederacy . His wife , Martha '\n",
      "                                             '( Suzy Amis ) , is a pretty but '\n",
      "                                             'strong - willed frontier woman , '\n",
      "                                             'whose independence makes her a '\n",
      "                                             'force in and',\n",
      "                         'start_index': 199,\n",
      "                         'start_logit': 0.1634644865989685,\n",
      "                         'start_stdev': 0.0,\n",
      "                         'target_type_logits': [0.22504714131355286,\n",
      "                                                -0.05453737452626228,\n",
      "                                                0.14259174466133118,\n",
      "                                                0.07970964163541794,\n",
      "                                                -0.4167461395263672],\n",
      "                         'yes_no_answer': 0},\n",
      "                        {'cls_score': -0.4857536405324936,\n",
      "                         'confidence_score': 0.33362213877701397,\n",
      "                         'end_index': 284,\n",
      "                         'end_logit': 0.14462396502494812,\n",
      "                         'end_stdev': 0.0,\n",
      "                         'example_id': '682243000439863277',\n",
      "                         'normalized_span_answer_score': 0.33362213877701397,\n",
      "                         'passage_index': -1,\n",
      "                         'query_passage_similarity': 0.0,\n",
      "                         'span_answer': {'end_position': 2112,\n",
      "                                         'start_position': 1727},\n",
      "                         'span_answer_score': 0.3675841744989157,\n",
      "                         'span_answer_text': '4 Reception * 4.1 Awards and '\n",
      "                                             'nominations * 5 References * 6 '\n",
      "                                             'External links Plot ( edit ) As '\n",
      "                                             'America recovers from the Civil '\n",
      "                                             'War , Paul Cable ( Tom Selleck ) '\n",
      "                                             'returns home to Texas after '\n",
      "                                             'being away from his family for '\n",
      "                                             'years while fighting for the '\n",
      "                                             'Confederacy . His wife , Martha '\n",
      "                                             '( Suzy Amis ) , is a pretty but '\n",
      "                                             'strong - willed frontier woman , '\n",
      "                                             'whose independence makes her a '\n",
      "                                             'force in and',\n",
      "                         'start_index': 188,\n",
      "                         'start_logit': 0.15932811796665192,\n",
      "                         'start_stdev': 0.0,\n",
      "                         'target_type_logits': [0.22504714131355286,\n",
      "                                                -0.05453737452626228,\n",
      "                                                0.14259174466133118,\n",
      "                                                0.07970964163541794,\n",
      "                                                -0.4167461395263672],\n",
      "                         'yes_no_answer': 0},\n",
      "                        {'cls_score': -0.4857536405324936,\n",
      "                         'confidence_score': 0.3320650163634881,\n",
      "                         'end_index': 284,\n",
      "                         'end_logit': 0.14462396502494812,\n",
      "                         'end_stdev': 0.0,\n",
      "                         'example_id': '682243000439863277',\n",
      "                         'normalized_span_answer_score': 0.3320650163634881,\n",
      "                         'passage_index': -1,\n",
      "                         'query_passage_similarity': 0.0,\n",
      "                         'span_answer': {'end_position': 2112,\n",
      "                                         'start_position': 1517},\n",
      "                         'span_answer_score': 0.3629059251397848,\n",
      "                         'span_answer_text': 'Actor in a Made For TV Movie . '\n",
      "                                             'In 1998 , the film received the '\n",
      "                                             'Western Heritage Awards Bronze '\n",
      "                                             'Wrangler for Television Feature '\n",
      "                                             'Film . Contents ( hide ) * 1 '\n",
      "                                             'Plot * 2 Cast * 3 Production * '\n",
      "                                             '3.1 Filming locations * 4 '\n",
      "                                             'Reception * 4.1 Awards and '\n",
      "                                             'nominations * 5 References * 6 '\n",
      "                                             'External links Plot ( edit ) As '\n",
      "                                             'America recovers from the Civil '\n",
      "                                             'War , Paul Cable ( Tom Selleck ) '\n",
      "                                             'returns home to Texas after '\n",
      "                                             'being away from his family for '\n",
      "                                             'years while fighting for the '\n",
      "                                             'Confederacy . His wife , Martha '\n",
      "                                             '( Suzy Amis ) , is a pretty but '\n",
      "                                             'strong - willed frontier woman , '\n",
      "                                             'whose independence makes her a '\n",
      "                                             'force in and',\n",
      "                         'start_index': 135,\n",
      "                         'start_logit': 0.1499716192483902,\n",
      "                         'start_stdev': 0.0,\n",
      "                         'target_type_logits': [0.22504714131355286,\n",
      "                                                -0.05453737452626228,\n",
      "                                                0.14259174466133118,\n",
      "                                                0.07970964163541794,\n",
      "                                                -0.4167461395263672],\n",
      "                         'yes_no_answer': 0}],\n",
      " '7579233308152571691': [{'cls_score': -0.49815627932548523,\n",
      "                          'confidence_score': 0.33554094398062745,\n",
      "                          'end_index': 385,\n",
      "                          'end_logit': 0.1155007854104042,\n",
      "                          'end_stdev': 0.0,\n",
      "                          'example_id': '7579233308152571691',\n",
      "                          'normalized_span_answer_score': 0.33554094398062745,\n",
      "                          'passage_index': -1,\n",
      "                          'query_passage_similarity': 0.0,\n",
      "                          'span_answer': {'end_position': 9061,\n",
      "                                          'start_position': 8125},\n",
      "                          'span_answer_score': 0.3815545253455639,\n",
      "                          'span_answer_text': 'resource that can be used to '\n",
      "                                              'generate electricity , as with '\n",
      "                                              'these 5MW wind turbines in '\n",
      "                                              'Thorntonbank Wind Farm 28 km ( '\n",
      "                                              '17 mi ) off the coast of '\n",
      "                                              'Belgium See also : Exploitation '\n",
      "                                              'of natural resources In recent '\n",
      "                                              'years , the depletion of '\n",
      "                                              'natural resources has become a '\n",
      "                                              'major focus of governments and '\n",
      "                                              'organizations such as the '\n",
      "                                              'United Nations ( UN ) . This is '\n",
      "                                              \"evident in the UN 's Agenda 21 \"\n",
      "                                              'Section Two , which outlines '\n",
      "                                              'the necessary steps to be taken '\n",
      "                                              'by countries to sustain their '\n",
      "                                              'natural resources . The '\n",
      "                                              'depletion of natural resources '\n",
      "                                              'is considered to be a '\n",
      "                                              'sustainable development issue . '\n",
      "                                              'The term sustainable '\n",
      "                                              'development has many '\n",
      "                                              'interpretations , most notably '\n",
      "                                              \"the Brundtland Commission 's ' \"\n",
      "                                              'to ensure that it meets the '\n",
      "                                              'needs of the present without '\n",
      "                                              'compromising the ability of '\n",
      "                                              'future generations to meet '\n",
      "                                              \"their own needs ' , however in \"\n",
      "                                              'broad terms it is balancing the '\n",
      "                                              \"needs of the planet 's people \"\n",
      "                                              'and species now and in the '\n",
      "                                              'future . In regard',\n",
      "                          'start_index': 183,\n",
      "                          'start_logit': 0.20261569321155548,\n",
      "                          'start_stdev': 0.0,\n",
      "                          'target_type_logits': [0.21436139941215515,\n",
      "                                                 -0.05316370725631714,\n",
      "                                                 0.14953279495239258,\n",
      "                                                 0.06939984112977982,\n",
      "                                                 -0.4186624586582184],\n",
      "                          'yes_no_answer': 0},\n",
      "                         {'cls_score': -0.48314401507377625,\n",
      "                          'confidence_score': 0.33373909624049225,\n",
      "                          'end_index': 454,\n",
      "                          'end_logit': 0.07641945779323578,\n",
      "                          'end_stdev': 0.0,\n",
      "                          'example_id': '7579233308152571691',\n",
      "                          'normalized_span_answer_score': 0.33373909624049225,\n",
      "                          'passage_index': -1,\n",
      "                          'query_passage_similarity': 0.0,\n",
      "                          'span_answer': {'end_position': 20919,\n",
      "                                          'start_position': 20319},\n",
      "                          'span_answer_score': 0.3761700764298439,\n",
      "                          'span_answer_text': 'Resource Management . pp. 1 -- '\n",
      "                                              '22 . * Jump up ^ Anthony , '\n",
      "                                              'Craig ( 12 September 2016 ) . '\n",
      "                                              '`` 10 Countries With The Most '\n",
      "                                              \"Natural Resources '' . \"\n",
      "                                              'Investopedia . External links ( '\n",
      "                                              'edit ) * Media related to '\n",
      "                                              'Natural resources at Wikimedia '\n",
      "                                              'Commons ( hide ) * * * Natural '\n",
      "                                              'resources Air Pollution / '\n",
      "                                              'quality * Ambient standards ( '\n",
      "                                              'USA ) * Index * Indoor * '\n",
      "                                              'developing nations * Law * '\n",
      "                                              'Clean Air Act ( USA ) * Ozone '\n",
      "                                              'depletion Emissions * Airshed * '\n",
      "                                              'Trading * Deforestation ( REDD '\n",
      "                                              ') Energy * Law * Resources * '\n",
      "                                              'Fossil fuels ( peak oil ) * '\n",
      "                                              'Geothermal * Nuclear * Solar * '\n",
      "                                              'sunlight * shade * Tidal * Wave '\n",
      "                                              '* Wind Land * Arab',\n",
      "                          'start_index': 293,\n",
      "                          'start_logit': 0.2430090457201004,\n",
      "                          'start_stdev': 0.0,\n",
      "                          'target_type_logits': [0.20881256461143494,\n",
      "                                                 -0.05023236572742462,\n",
      "                                                 0.15056630969047546,\n",
      "                                                 0.06760223954916,\n",
      "                                                 -0.41381359100341797],\n",
      "                          'yes_no_answer': 0},\n",
      "                         {'cls_score': -0.48293814063072205,\n",
      "                          'confidence_score': 0.33071995977888036,\n",
      "                          'end_index': 284,\n",
      "                          'end_logit': 0.027920346707105637,\n",
      "                          'end_stdev': 0.0,\n",
      "                          'example_id': '7579233308152571691',\n",
      "                          'normalized_span_answer_score': 0.33071995977888036,\n",
      "                          'passage_index': -1,\n",
      "                          'query_passage_similarity': 0.0,\n",
      "                          'span_answer': {'end_position': 24106,\n",
      "                                          'start_position': 23704},\n",
      "                          'span_answer_score': 0.3670825120061636,\n",
      "                          'span_answer_text': 'Türkçe * Українська * Tiếng '\n",
      "                                              'Việt * Winaray * 粵語 * 中文 61 '\n",
      "                                              'more Edit links * This page was '\n",
      "                                              'last edited on 19 April 2018 , '\n",
      "                                              'at 00 : 03 . * Text is '\n",
      "                                              'available under the Creative '\n",
      "                                              'Commons Attribution - '\n",
      "                                              'ShareAlike License ; additional '\n",
      "                                              'terms may apply . By using this '\n",
      "                                              'site , you agree to the Terms '\n",
      "                                              'of Use and Privacy Policy . '\n",
      "                                              'Wikipedia ® is a registered '\n",
      "                                              'trademark of the Wikimedia '\n",
      "                                              'Foundation , Inc. , a '\n",
      "                                              'non-profit',\n",
      "                          'start_index': 186,\n",
      "                          'start_logit': 0.264978289604187,\n",
      "                          'start_stdev': 0.0,\n",
      "                          'target_type_logits': [0.2174028605222702,\n",
      "                                                 -0.0416717529296875,\n",
      "                                                 0.13276946544647217,\n",
      "                                                 0.06257370114326477,\n",
      "                                                 -0.41713762283325195],\n",
      "                          'yes_no_answer': 0}]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "with open(os.path.join(output_dir, 'eval_predictions.json'), 'r') as f:\n",
    "    predictions = json.load(f)\n",
    "\n",
    "pprint(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('primeqaenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "229bd894a0cdb05b7ee80ea2bc43559a301775857073a25e64e4f441f37822ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
